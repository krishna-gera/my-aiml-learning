{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNnvDx6KINZOUtKTbxpnIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-20/day20_shap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pah0RhXGr99a",
        "outputId": "0a2f301a-dbbb-433e-c33b-3f570873f736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shapes — train: (712, 29) val: (179, 29)\n",
            "No saved pipeline found. Training a quick stacking pipeline...\n",
            "Saved new pipeline to: models/day20_20250927_1501_stacking_pipeline.pkl\n",
            "Trying SHAP on RandomForest base model...\n",
            "SHAP TreeExplainer failed: Per-column arrays must each be 1-dimensional\n",
            "SHAP not used or failed — computing permutation importance instead.\n",
            "Computing permutation importance (model-agnostic)...\n",
            "Saved permutation importance to: outputs/day20_20250927_1501_permutation_importance.csv\n",
            "Running threshold sweep on validation set...\n",
            "Saved threshold sweep to: outputs/day20_20250927_1501_thresholds.csv\n",
            "Best threshold (by accuracy): {'threshold': 0.49999999999999994, 'accuracy': 0.8491620111731844, 'f1': 0.7969924812030075}\n",
            "Calibrating pipeline (CalibratedClassifierCV with sigmoid)...\n",
            "Calibration failed: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'\n",
            "Saved summary to: outputs/day20_20250927_1501_summary.csv\n",
            "\n",
            "DONE — check outputs/ for CSVs and plots, models/ for pipeline (if created).\n"
          ]
        }
      ],
      "source": [
        "# day20_shap.py\n",
        "# Day 20 — SHAP explainability, permutation importance fallback, threshold sweep\n",
        "# Save as day20_shap.py and run: python day20_shap.py\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, brier_score_loss\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "TRAIN_CSV = DATA_DIR / \"train_processed.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test_processed.csv\"\n",
        "\n",
        "MODELS_DIR = Path(\"models\")\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "HOLDOUT_SIZE = 0.20\n",
        "CV_FOLDS = 5\n",
        "# ---------------------------------------\n",
        "\n",
        "# safe OHE factory for sklearn versions\n",
        "def make_ohe():\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    else:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def find_latest_pipeline():\n",
        "    files = sorted(glob.glob(str(MODELS_DIR / \"*stack*.pkl\")))\n",
        "    if not files:\n",
        "        return None\n",
        "    return files[-1]\n",
        "\n",
        "def get_preprocessor_from_pipeline(pipeline):\n",
        "    for name in (\"preprocessor\", \"pre\", \"preprocessing\", \"prep\"):\n",
        "        if name in pipeline.named_steps:\n",
        "            return pipeline.named_steps[name]\n",
        "    # fallback: search for a ColumnTransformer in named_steps\n",
        "    for step in pipeline.named_steps.values():\n",
        "        if isinstance(step, ColumnTransformer):\n",
        "            return step\n",
        "    return None\n",
        "\n",
        "def get_stack_from_pipeline(pipeline):\n",
        "    for name in (\"stack\", \"stacking\", \"stacker\", \"stackingclassifier\", \"stacking_clf\"):\n",
        "        if name in pipeline.named_steps:\n",
        "            return pipeline.named_steps[name]\n",
        "    # fallback search for StackingClassifier instance\n",
        "    for step in pipeline.named_steps.values():\n",
        "        if hasattr(step, \"estimators_\") or step.__class__.__name__.lower().startswith(\"stack\"):\n",
        "            return step\n",
        "    return None\n",
        "\n",
        "def build_quick_pipeline(X_df):\n",
        "    # ensure bools converted to strings (avoid imputer dtype issues)\n",
        "    X_copy = X_df.copy()\n",
        "    for col in X_copy.select_dtypes(include=[\"bool\"]).columns:\n",
        "        X_copy[col] = X_copy[col].astype(str)\n",
        "\n",
        "    numeric_features = X_copy.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "    categorical_features = X_copy.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "    # remove passenger id if present\n",
        "    for c in (\"PassengerId\",):\n",
        "        if c in numeric_features:\n",
        "            numeric_features.remove(c)\n",
        "        if c in categorical_features:\n",
        "            categorical_features.remove(c)\n",
        "\n",
        "    num_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    cat_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
        "        (\"ohe\", make_ohe())\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", num_transformer, numeric_features),\n",
        "        (\"cat\", cat_transformer, categorical_features)\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=RANDOM_STATE)\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "        estimators=[(\"rf\", rf), (\"gb\", gb)],\n",
        "        final_estimator=LogisticRegression(max_iter=2000),\n",
        "        passthrough=True,\n",
        "        cv=5, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"pre\", preprocessor),\n",
        "        (\"stack\", stack)\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "def safe_get_feature_names(preprocessor, X_df):\n",
        "    # Try several ways to retrieve feature names after ColumnTransformer\n",
        "    try:\n",
        "        # preferred: pass input feature names\n",
        "        return list(preprocessor.get_feature_names_out(X_df.columns))\n",
        "    except Exception:\n",
        "        try:\n",
        "            return list(preprocessor.get_feature_names_out())\n",
        "        except Exception:\n",
        "            # fallback: return generic f0..fN\n",
        "            # we can estimate length by transforming one row\n",
        "            try:\n",
        "                arr = preprocessor.transform(X_df.iloc[:1])\n",
        "                return [f\"f{i}\" for i in range(arr.shape[1])]\n",
        "            except Exception:\n",
        "                return []\n",
        "\n",
        "def try_shap_on_rf(rf_model, preprocessor, X_val, feature_names, out_prefix):\n",
        "    \"\"\"Attempt SHAP TreeExplainer on RF model. Save CSV + plot. Return True on success.\"\"\"\n",
        "    try:\n",
        "        import shap\n",
        "    except Exception as e:\n",
        "        print(\"shap not installed or import failed:\", e)\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Transform validation data\n",
        "        X_val_pre = preprocessor.transform(X_val)\n",
        "        # Ensure numpy float\n",
        "        X_val_pre = np.asarray(X_val_pre, dtype=float)\n",
        "\n",
        "        # TreeExplainer accepts the tree model and raw numpy\n",
        "        explainer = shap.TreeExplainer(rf_model)\n",
        "        shap_values = explainer.shap_values(X_val_pre)  # list for classes or array\n",
        "\n",
        "        # For binary classifier shap_values is list-like: take class 1\n",
        "        if isinstance(shap_values, list) and len(shap_values) > 1:\n",
        "            shap_vals = shap_values[1]\n",
        "        else:\n",
        "            shap_vals = shap_values\n",
        "\n",
        "        # mean abs shap\n",
        "        mean_abs = np.abs(shap_vals).mean(axis=0)\n",
        "        df = pd.DataFrame({\"feature\": feature_names, \"mean_abs_shap\": mean_abs})\n",
        "        df = df.sort_values(\"mean_abs_shap\", ascending=False)\n",
        "        out_csv = OUT_DIR / f\"{out_prefix}_shap_importance.csv\"\n",
        "        df.to_csv(out_csv, index=False)\n",
        "        print(\"Saved SHAP importances to:\", out_csv)\n",
        "\n",
        "        # produce a summary plot and save\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        shap.summary_plot(shap_vals, features=X_val_pre, feature_names=feature_names, show=False)\n",
        "        plt.tight_layout()\n",
        "        out_png = OUT_DIR / f\"{out_prefix}_shap_summary.png\"\n",
        "        plt.savefig(out_png, dpi=200)\n",
        "        plt.close()\n",
        "        print(\"Saved SHAP summary plot to:\", out_png)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"SHAP TreeExplainer failed:\", e)\n",
        "        return False\n",
        "\n",
        "def permutation_importance_and_save(pipeline, X_val, y_val, preprocessor, X_df, out_prefix):\n",
        "    print(\"Computing permutation importance (model-agnostic)...\")\n",
        "    r = permutation_importance(pipeline, X_val, y_val, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    # feature names\n",
        "    feat_names = safe_get_feature_names(preprocessor, X_df)\n",
        "    # align length\n",
        "    L = len(r.importances_mean)\n",
        "    if len(feat_names) != L:\n",
        "        # fallback name generation\n",
        "        feat_names = [f\"f{i}\" for i in range(L)]\n",
        "    df = pd.DataFrame({\n",
        "        \"feature\": feat_names,\n",
        "        \"importance_mean\": r.importances_mean,\n",
        "        \"importance_std\": r.importances_std\n",
        "    }).sort_values(\"importance_mean\", ascending=False)\n",
        "    out_csv = OUT_DIR / f\"{out_prefix}_permutation_importance.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"Saved permutation importance to:\", out_csv)\n",
        "    return df\n",
        "\n",
        "def threshold_sweep(pipeline, X_val, y_val, out_prefix):\n",
        "    print(\"Running threshold sweep on validation set...\")\n",
        "    # get probabilities\n",
        "    if hasattr(pipeline, \"predict_proba\"):\n",
        "        probs = pipeline.predict_proba(X_val)[:, 1]\n",
        "    else:\n",
        "        # try accessing inner stack\n",
        "        probs = pipeline.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    thresholds = np.linspace(0.05, 0.95, 19)\n",
        "    recs = []\n",
        "    best = {\"threshold\": None, \"accuracy\": -1, \"f1\": -1}\n",
        "    for t in thresholds:\n",
        "        preds = (probs >= t).astype(int)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        f1 = f1_score(y_val, preds)\n",
        "        recs.append({\"threshold\": float(t), \"accuracy\": float(acc), \"f1\": float(f1)})\n",
        "        if acc > best[\"accuracy\"]:\n",
        "            best = {\"threshold\": float(t), \"accuracy\": float(acc), \"f1\": float(f1)}\n",
        "    df = pd.DataFrame(recs)\n",
        "    out_csv = OUT_DIR / f\"{out_prefix}_thresholds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"Saved threshold sweep to:\", out_csv)\n",
        "    print(\"Best threshold (by accuracy):\", best)\n",
        "    return best, df\n",
        "\n",
        "def calibrate_pipeline_and_eval(pipeline, X_train, y_train, X_val, y_val, out_prefix):\n",
        "    print(\"Calibrating pipeline (CalibratedClassifierCV with sigmoid)...\")\n",
        "    try:\n",
        "        cal = CalibratedClassifierCV(base_estimator=pipeline, method=\"sigmoid\", cv=3)\n",
        "        cal.fit(X_train, y_train)\n",
        "        probs = cal.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        brier = brier_score_loss(y_val, probs)\n",
        "        metrics = {\"calibrated_accuracy\": float(acc), \"brier_score\": float(brier)}\n",
        "        out_csv = OUT_DIR / f\"{out_prefix}_calibration_metrics.csv\"\n",
        "        pd.Series(metrics).to_frame(\"value\").to_csv(out_csv)\n",
        "        joblib.dump(cal, MODELS_DIR / f\"{out_prefix}_calibrated.pkl\")\n",
        "        print(\"Saved calibration metrics and calibrated model.\")\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(\"Calibration failed:\", e)\n",
        "        return None\n",
        "\n",
        "# ========== MAIN ==========\n",
        "def main():\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    prefix = f\"day20_{ts}\"\n",
        "\n",
        "    # load data\n",
        "    train = pd.read_csv(TRAIN_CSV)\n",
        "    test = pd.read_csv(TEST_CSV)\n",
        "\n",
        "    # safety: convert bools -> str early\n",
        "    for df in (train, test):\n",
        "        for c in df.select_dtypes(include=[\"bool\"]).columns:\n",
        "            df[c] = df[c].astype(str)\n",
        "\n",
        "    if \"Survived\" not in train.columns:\n",
        "        raise SystemExit(\"train_processed.csv must contain 'Survived' column\")\n",
        "\n",
        "    X_full = train.drop(columns=[\"Survived\"])\n",
        "    y_full = train[\"Survived\"].astype(int)\n",
        "\n",
        "    # holdout split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=HOLDOUT_SIZE, stratify=y_full, random_state=RANDOM_STATE)\n",
        "    print(\"Data shapes — train:\", X_train.shape, \"val:\", X_val.shape)\n",
        "\n",
        "    # 1) Load or train pipeline\n",
        "    latest = find_latest_pipeline()\n",
        "    if latest:\n",
        "        print(\"Loading pipeline:\", latest)\n",
        "        pipeline = joblib.load(latest)\n",
        "        pipeline_source = latest\n",
        "    else:\n",
        "        print(\"No saved pipeline found. Training a quick stacking pipeline...\")\n",
        "        pipeline = build_quick_pipeline(X_train)\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        save_path = MODELS_DIR / f\"{prefix}_stacking_pipeline.pkl\"\n",
        "        joblib.dump(pipeline, save_path)\n",
        "        pipeline_source = str(save_path)\n",
        "        print(\"Saved new pipeline to:\", save_path)\n",
        "\n",
        "    # get preprocessor and stack\n",
        "    pre = get_preprocessor_from_pipeline(pipeline)\n",
        "    stack = get_stack_from_pipeline(pipeline)\n",
        "\n",
        "    if pre is None:\n",
        "        raise SystemExit(\"Could not find preprocessor inside pipeline (expected ColumnTransformer).\")\n",
        "\n",
        "    # feature names\n",
        "    feat_names = safe_get_feature_names(pre, X_train)\n",
        "    if len(feat_names) == 0:\n",
        "        print(\"Warning: couldn't determine feature names. Some outputs may use generic names.\")\n",
        "\n",
        "    # 2) Try SHAP on RF inside stack\n",
        "    rf_model = None\n",
        "    try:\n",
        "        if hasattr(stack, \"named_estimators_\"):\n",
        "            rf_model = stack.named_estimators_.get(\"rf\", None)\n",
        "        # fallback: if estimators_ attribute\n",
        "        if rf_model is None and hasattr(stack, \"estimators_\"):\n",
        "            # estimators_ is list of fitted models\n",
        "            for name, est in stack.estimators_:\n",
        "                if name == \"rf\":\n",
        "                    rf_model = est\n",
        "                    break\n",
        "    except Exception:\n",
        "        rf_model = None\n",
        "\n",
        "    shap_success = False\n",
        "    if rf_model is not None:\n",
        "        print(\"Trying SHAP on RandomForest base model...\")\n",
        "        try:\n",
        "            shap_success = try_shap_on_rf(rf_model, pre, X_val, feat_names, out_prefix=prefix)\n",
        "        except Exception as e:\n",
        "            print(\"SHAP attempt raised an error:\", e)\n",
        "            shap_success = False\n",
        "    else:\n",
        "        print(\"No RF found inside stacked model — skipping SHAP attempt.\")\n",
        "\n",
        "    # 3) If SHAP failed or unavailable, compute permutation importance\n",
        "    if not shap_success:\n",
        "        print(\"SHAP not used or failed — computing permutation importance instead.\")\n",
        "        perm_df = permutation_importance_and_save(pipeline, X_val, y_val, pre, X_val, out_prefix=prefix)\n",
        "    else:\n",
        "        perm_df = None\n",
        "\n",
        "    # 4) Threshold sweep and save\n",
        "    best_threshold, thresholds_df = threshold_sweep(pipeline, X_val, y_val, out_prefix=prefix)\n",
        "\n",
        "    # 5) Optional calibration (quick)\n",
        "    calibration = calibrate_pipeline_and_eval(pipeline, X_train, y_train, X_val, y_val, out_prefix=prefix)\n",
        "\n",
        "    # 6) Save summary\n",
        "    summary = {\n",
        "        \"timestamp\": ts,\n",
        "        \"pipeline_source\": pipeline_source,\n",
        "        \"shap_used\": bool(shap_success),\n",
        "        \"best_threshold\": best_threshold,\n",
        "        \"calibration\": calibration\n",
        "    }\n",
        "    pd.Series(summary).to_frame(\"value\").to_csv(OUT_DIR / f\"{prefix}_summary.csv\")\n",
        "    print(\"Saved summary to:\", OUT_DIR / f\"{prefix}_summary.csv\")\n",
        "\n",
        "    print(\"\\nDONE — check outputs/ for CSVs and plots, models/ for pipeline (if created).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}