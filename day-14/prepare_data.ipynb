{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxN6cNiWx3Xl90Il4v9vHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-14/prepare_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare_data.py - error-free preprocessing for Titanic\n",
        "# - reads:  data/train.csv  and  data/test.csv\n",
        "# - writes: data/processed/train_processed.csv  and  data/processed/test_processed.csv\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "RAW_DIR = Path(\"data\")\n",
        "OUT_DIR = Path(\"data/processed\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# 0) sanity check - files present\n",
        "# -------------------------\n",
        "train_path = RAW_DIR / \"train.csv\"\n",
        "test_path = RAW_DIR / \"test.csv\"\n",
        "\n",
        "if not train_path.exists():\n",
        "    raise FileNotFoundError(f\"train.csv not found at {train_path}. Put Kaggle train.csv in the 'data' folder.\")\n",
        "if not test_path.exists():\n",
        "    raise FileNotFoundError(f\"test.csv not found at {test_path}. Put Kaggle test.csv in the 'data' folder.\")\n",
        "\n",
        "# -------------------------\n",
        "# 1) load raw files\n",
        "# -------------------------\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print(\"Raw shapes -> train:\", train.shape, \" test:\", test.shape)\n",
        "\n",
        "# preserve PassengerId for final submission\n",
        "test_passenger_id = test[\"PassengerId\"].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 2) combine train+test so dummies/encoding align\n",
        "# -------------------------\n",
        "train[\"is_train\"] = 1\n",
        "test[\"is_train\"] = 0\n",
        "# Ensure Survived exists in test to keep columns consistent (fill with NaN)\n",
        "if \"Survived\" not in test.columns:\n",
        "    test[\"Survived\"] = np.nan\n",
        "\n",
        "full = pd.concat([train, test], ignore_index=True, sort=False)\n",
        "print(\"Combined shape:\", full.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 3) feature engineering on combined set\n",
        "# -------------------------\n",
        "\n",
        "# Title extraction (use raw string to avoid escape warnings)\n",
        "full[\"Title\"] = full[\"Name\"].str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n",
        "\n",
        "# Clean titles and collapse rare ones\n",
        "full[\"Title\"] = full[\"Title\"].replace({\n",
        "    \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n",
        "    \"Lady\":\"Rare\",\"Countess\":\"Rare\",\"Capt\":\"Rare\",\"Col\":\"Rare\",\"Don\":\"Rare\",\n",
        "    \"Dr\":\"Rare\",\"Major\":\"Rare\",\"Rev\":\"Rare\",\"Sir\":\"Rare\",\"Jonkheer\":\"Rare\",\"Dona\":\"Rare\"\n",
        "})\n",
        "# mark very rare titles as 'Rare'\n",
        "title_counts = full[\"Title\"].value_counts(dropna=True)\n",
        "rare_titles = title_counts[title_counts < 10].index.tolist()\n",
        "full[\"Title\"] = full[\"Title\"].replace(rare_titles, \"Rare\")\n",
        "full[\"Title\"] = full[\"Title\"].fillna(\"Unknown\")\n",
        "\n",
        "# Family features\n",
        "full[\"FamilySize\"] = full[\"SibSp\"].fillna(0).astype(int) + full[\"Parch\"].fillna(0).astype(int) + 1\n",
        "full[\"IsAlone\"] = (full[\"FamilySize\"] == 1).astype(int)\n",
        "\n",
        "# Fare: fill missing then log transform\n",
        "full[\"Fare\"] = pd.to_numeric(full[\"Fare\"], errors=\"coerce\")\n",
        "full[\"Fare\"].fillna(full[\"Fare\"].median(), inplace=True)\n",
        "full[\"Fare_log\"] = np.log1p(full[\"Fare\"])\n",
        "\n",
        "# Age: fill missing by median age per Title, fallback to global median\n",
        "full[\"Age\"] = pd.to_numeric(full[\"Age\"], errors=\"coerce\")\n",
        "age_median_by_title = full.groupby(\"Title\")[\"Age\"].transform(\"median\")\n",
        "global_age_median = full[\"Age\"].median()\n",
        "full[\"Age\"] = full[\"Age\"].fillna(age_median_by_title)\n",
        "full[\"Age\"] = full[\"Age\"].fillna(global_age_median)\n",
        "\n",
        "# Age bins (optional numeric category)\n",
        "full[\"AgeBin\"] = pd.cut(full[\"Age\"], bins=[0,12,20,40,60,120], labels=[\"Child\",\"Teen\",\"Adult\",\"MidAge\",\"Senior\"])\n",
        "full[\"AgeBin\"] = full[\"AgeBin\"].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "# Embarked: fill with mode\n",
        "if \"Embarked\" in full.columns:\n",
        "    full[\"Embarked\"] = full[\"Embarked\"].fillna(full[\"Embarked\"].mode().iloc[0])\n",
        "\n",
        "# Deck from Cabin - take first letter, fill 'U' for unknown\n",
        "if \"Cabin\" in full.columns:\n",
        "    full[\"Deck\"] = full[\"Cabin\"].astype(str).str[0].replace(\"n\", \"U\")  # guard vs NaN -> 'n'\n",
        "    full[\"Deck\"] = full[\"Deck\"].replace(\"N\", \"U\").fillna(\"U\")\n",
        "else:\n",
        "    full[\"Deck\"] = \"U\"\n",
        "\n",
        "# Ticket group (group size) - optional\n",
        "if \"Ticket\" in full.columns:\n",
        "    full[\"Ticket\"] = full[\"Ticket\"].fillna(\"NA\")\n",
        "    ticket_counts = full[\"Ticket\"].map(full[\"Ticket\"].value_counts())\n",
        "    full[\"TicketGroupSize\"] = ticket_counts.fillna(1).astype(int)\n",
        "else:\n",
        "    full[\"TicketGroupSize\"] = 1\n",
        "\n",
        "# Drop columns we won't use directly (Name/Cabin/Ticket) BEFORE get_dummies\n",
        "drop_cols = [c for c in [\"Name\",\"Ticket\",\"Cabin\"] if c in full.columns]\n",
        "full = full.drop(columns=drop_cols)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Encoding: one-hot on categorical columns (do on combined so both sets match)\n",
        "# -------------------------\n",
        "# Choose categorical cols to one-hot: Sex, Embarked, Title, Deck, AgeBin (only those that exist)\n",
        "cat_cols = [c for c in [\"Sex\",\"Embarked\",\"Title\",\"Deck\",\"AgeBin\"] if c in full.columns]\n",
        "full = pd.get_dummies(full, columns=cat_cols, drop_first=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5) split back into train/test\n",
        "# -------------------------\n",
        "train_proc = full[full[\"is_train\"] == 1].copy().drop(columns=[\"is_train\"])\n",
        "test_proc = full[full[\"is_train\"] == 0].copy().drop(columns=[\"is_train\"])\n",
        "\n",
        "# Ensure Survived present only in train_proc (it will be NaN in test_proc) -> remove in test_proc\n",
        "if \"Survived\" in test_proc.columns:\n",
        "    test_proc = test_proc.drop(columns=[\"Survived\"])\n",
        "\n",
        "# -------------------------\n",
        "# 6) ensure PassengerId preserved in test_proc\n",
        "# -------------------------\n",
        "# if PassengerId exists already, overwrite with original saved values to be safe\n",
        "test_proc[\"PassengerId\"] = test_passenger_id.values\n",
        "\n",
        "# put PassengerId as first column\n",
        "cols = list(test_proc.columns)\n",
        "if cols[0] != \"PassengerId\":\n",
        "    cols.remove(\"PassengerId\")\n",
        "    cols = [\"PassengerId\"] + cols\n",
        "    test_proc = test_proc[cols]\n",
        "\n",
        "# -------------------------\n",
        "# 7) Align columns: make sure test_proc has exactly the same feature columns (except Survived)\n",
        "# -------------------------\n",
        "# For safety, get the feature columns from train_proc (excluding Survived)\n",
        "train_feature_cols = [c for c in train_proc.columns if c != \"Survived\"]\n",
        "\n",
        "# If some train feature columns are missing from test (rare), add them as zeros\n",
        "for c in train_feature_cols:\n",
        "    if c not in test_proc.columns:\n",
        "        test_proc[c] = 0\n",
        "\n",
        "# Reorder test_proc columns to match train feature order, but keep PassengerId first\n",
        "ordered_test_cols = [\"PassengerId\"] + [c for c in train_feature_cols if c in test_proc.columns and c != \"PassengerId\"]\n",
        "test_proc = test_proc[ordered_test_cols]\n",
        "\n",
        "# If test has extra columns not in train (shouldn't happen since we used combined dummies), drop them (except PassengerId)\n",
        "extra_cols = [c for c in test_proc.columns if c not in ([\"PassengerId\"] + train_feature_cols)]\n",
        "if extra_cols:\n",
        "    # keep PassengerId and drop others\n",
        "    drop_these = [c for c in extra_cols if c != \"PassengerId\"]\n",
        "    if drop_these:\n",
        "        test_proc = test_proc.drop(columns=drop_these)\n",
        "\n",
        "# Final train_proc: keep Survived and feature columns\n",
        "train_proc = train_proc[[\"Survived\"] + [c for c in train_feature_cols if c != \"Survived\"]]\n",
        "\n",
        "# -------------------------\n",
        "# 8) Save outputs\n",
        "# -------------------------\n",
        "train_out = OUT_DIR / \"train_processed.csv\"\n",
        "test_out = OUT_DIR / \"test_processed.csv\"\n",
        "\n",
        "train_proc.to_csv(train_out, index=False)\n",
        "test_proc.to_csv(test_out, index=False)\n",
        "\n",
        "print(\"✅ Saved processed files:\")\n",
        "print(\" -\", train_out, \" shape:\", train_proc.shape)\n",
        "print(\" -\", test_out, \" shape:\", test_proc.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBZaWNtsLlHR",
        "outputId": "d3831e97-fd78-4006-c140-db88eb6c4b70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shapes -> train: (891, 12)  test: (418, 11)\n",
            "Combined shape: (1309, 13)\n",
            "✅ Saved processed files:\n",
            " - data/processed/train_processed.csv  shape: (891, 30)\n",
            " - data/processed/test_processed.csv  shape: (418, 29)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1451359259.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  full[\"Fare\"].fillna(full[\"Fare\"].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Paths\n",
        "# ---------------------------\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "RAW_DIR = Path(\"data/\")\n",
        "SUB_DIR = Path(\"submissions\")\n",
        "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_path = DATA_DIR / \"train_processed.csv\"\n",
        "test_path = DATA_DIR / \"test_processed.csv\"\n",
        "raw_test_path = RAW_DIR / \"test.csv\"  # original test.csv for correct PassengerId order\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Load data\n",
        "# ---------------------------\n",
        "train = pd.read_csv(train_path)\n",
        "test_processed = pd.read_csv(test_path)\n",
        "test_raw = pd.read_csv(raw_test_path)  # use for PassengerId\n",
        "\n",
        "# Separate target and features\n",
        "X = train.drop([\"Survived\", \"PassengerId\"], axis=1)\n",
        "y = train[\"Survived\"]\n",
        "X_test = test_processed.drop(\"PassengerId\", axis=1)\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Train/Validation split\n",
        "# ---------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Define base models\n",
        "# ---------------------------\n",
        "base_models = [\n",
        "    (\"rf\", RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)),\n",
        "    (\"gb\", GradientBoostingClassifier(n_estimators=200, random_state=42)),\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Define stacking model\n",
        "# ---------------------------\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=False,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Train & Validate\n",
        "# ---------------------------\n",
        "stack_model.fit(X_train, y_train)\n",
        "val_pred = stack_model.predict(X_val)\n",
        "\n",
        "val_acc = accuracy_score(y_val, val_pred)\n",
        "print(\"✅ Validation Accuracy:\", val_acc)\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Predict on test set\n",
        "# ---------------------------\n",
        "test_pred = stack_model.predict(X_test)\n",
        "test_pred = test_pred.astype(int)  # ensure 0 or 1 for Kaggle\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Save submission (Kaggle-ready)\n",
        "# ---------------------------\n",
        "submission = pd.DataFrame({\n",
        "    \"PassengerId\": test_raw[\"PassengerId\"],  # original order!\n",
        "    \"Survived\": test_pred\n",
        "})\n",
        "\n",
        "out_path = SUB_DIR / \"day13_stacking_clean.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"✅ Submission saved at:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko9eeSFqOldD",
        "outputId": "dda71d44-4423-4fa7-cade-f985ee173d13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validation Accuracy: 0.8044692737430168\n",
            "✅ Submission saved at: submissions/day13_stacking_clean.csv\n"
          ]
        }
      ]
    }
  ]
}