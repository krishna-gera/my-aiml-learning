{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP2yziOSt0wu3q3tJp3Lqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-17/day17_tuning_quick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ad7wXFnK5F_",
        "outputId": "c84bd826-d9c0-4073-948e-6ca74da2ca3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 17 tuning — quick run\n",
            "Shape after encoding/imputation: (891, 30)\n",
            "\n",
            "Baseline RF CV accuracy: 0.8238 (+/- 0.0212)\n",
            "\n",
            "--- RandomizedSearchCV for random_forest (n_iter=12) ---\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best CV score (random_forest): 0.8451\n",
            "Best params (random_forest): {'n_estimators': 300, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_depth': 12, 'bootstrap': True}\n",
            "Saved RandomizedSearchCV object to models/random_forest_randomsearch_20250924_1443.pkl\n",
            "\n",
            "--- RandomizedSearchCV for xgboost (n_iter=12) ---\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best CV score (xgboost): 0.8485\n",
            "Best params (xgboost): {'subsample': 0.6, 'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
            "Saved RandomizedSearchCV object to models/xgboost_randomsearch_20250924_1443.pkl\n",
            "\n",
            "--- RandomizedSearchCV for lightgbm (n_iter=12) ---\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Info] Number of positive: 342, number of negative: 549\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 886\n",
            "[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 27\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288\n",
            "[LightGBM] [Info] Start training from score -0.473288\n",
            "Best CV score (lightgbm): 0.8395\n",
            "Best params (lightgbm): {'num_leaves': 15, 'n_estimators': 400, 'learning_rate': 0.03, 'feature_fraction': 1.0}\n",
            "Saved RandomizedSearchCV object to models/lightgbm_randomsearch_20250924_1443.pkl\n",
            "\n",
            "Done. Models saved in 'models/' directory.\n",
            "Next: • Use best estimators from these RandomizedSearchCV objects in your stacking pipeline.\n",
            "Tip: If run time is long, reduce N_ITER or n_estimators and try again.\n"
          ]
        }
      ],
      "source": [
        "# day17_tuning_quick.py\n",
        "\"\"\"\n",
        "Day 17 - Quick Hyperparameter Tuning (RandomizedSearchCV)\n",
        "- Tunes RandomForest (and XGBoost/LightGBM if available)\n",
        "- Designed to run fast for a 2-hour session (short n_iter)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib, datetime, warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "TRAIN_CSV = DATA_DIR / \"train_processed.csv\"\n",
        "MODELS_DIR = Path(\"models\")\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 5\n",
        "N_ITER = 12   # small for quick runs; increase later if you have more time\n",
        "VERBOSE = 2\n",
        "N_JOBS = -1\n",
        "# ----------------------------------------\n",
        "\n",
        "# Optional imports\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    HAS_LGB = True\n",
        "except Exception:\n",
        "    HAS_LGB = False\n",
        "\n",
        "def add_features_if_missing(df):\n",
        "    df = df.copy()\n",
        "    if \"FamilySize\" not in df.columns and {\"SibSp\", \"Parch\"}.issubset(df.columns):\n",
        "        df[\"FamilySize\"] = df[\"SibSp\"].fillna(0).astype(int) + df[\"Parch\"].fillna(0).astype(int) + 1\n",
        "    if \"IsAlone\" not in df.columns and \"FamilySize\" in df.columns:\n",
        "        df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
        "    if \"Title\" not in df.columns and \"Name\" in df.columns:\n",
        "        df[\"Title\"] = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\", expand=False).str.strip().fillna(\"Other\")\n",
        "    if \"FarePerPerson\" not in df.columns and \"Fare\" in df.columns:\n",
        "        df[\"FarePerPerson\"] = df[\"Fare\"].fillna(0) / df.get(\"FamilySize\", 1).replace(0,1)\n",
        "    return df\n",
        "\n",
        "def encode_and_impute(X_df):\n",
        "    X_enc = pd.get_dummies(X_df, dummy_na=False)\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X_imp = pd.DataFrame(imputer.fit_transform(X_enc), columns=X_enc.columns)\n",
        "    return X_imp, imputer, X_enc.columns.tolist()\n",
        "\n",
        "def cv_mean_std(clf, X, y, folds=CV_FOLDS):\n",
        "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_val_score(clf, X, y, cv=cv, scoring=\"accuracy\", n_jobs=N_JOBS)\n",
        "    return float(scores.mean()), float(scores.std()), scores\n",
        "\n",
        "def run_random_search(estimator, param_dist, X, y, name=\"model\"):\n",
        "    print(f\"\\n--- RandomizedSearchCV for {name} (n_iter={N_ITER}) ---\")\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    rs = RandomizedSearchCV(\n",
        "        estimator,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=N_ITER,\n",
        "        cv=cv,\n",
        "        scoring=\"accuracy\",\n",
        "        random_state=RANDOM_STATE,\n",
        "        verbose=VERBOSE,\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "    rs.fit(X, y)\n",
        "    print(f\"Best CV score ({name}): {rs.best_score_:.4f}\")\n",
        "    print(f\"Best params ({name}): {rs.best_params_}\")\n",
        "    # Save\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    joblib.dump(rs, MODELS_DIR / f\"{name}_randomsearch_{ts}.pkl\")\n",
        "    print(f\"Saved RandomizedSearchCV object to models/{name}_randomsearch_{ts}.pkl\")\n",
        "    return rs\n",
        "\n",
        "def main():\n",
        "    print(\"Day 17 tuning — quick run\")\n",
        "    train = pd.read_csv(TRAIN_CSV)\n",
        "    if \"Survived\" not in train.columns:\n",
        "        raise SystemExit(\"train_processed.csv must contain 'Survived' column\")\n",
        "    # Add features if missing (safe)\n",
        "    train = add_features_if_missing(train)\n",
        "    X_df = train.drop([\"Survived\"], axis=1, errors=\"ignore\")\n",
        "    y = train[\"Survived\"].astype(int)\n",
        "\n",
        "    # Encode & impute once (fast approach)\n",
        "    X, imputer, col_list = encode_and_impute(X_df)\n",
        "    print(\"Shape after encoding/imputation:\", X.shape)\n",
        "\n",
        "    # Baseline quick RF CV\n",
        "    base_rf = RandomForestClassifier(n_estimators=150, random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
        "    base_mean, base_std, base_scores = cv_mean_std(base_rf, X, y)\n",
        "    print(f\"\\nBaseline RF CV accuracy: {base_mean:.4f} (+/- {base_std:.4f})\")\n",
        "\n",
        "    # -------- RandomForest tuning --------\n",
        "    rf_param_dist = {\n",
        "        \"n_estimators\": [100, 200, 300, 400, 600],\n",
        "        \"max_depth\": [None, 5, 8, 12, 20],\n",
        "        \"min_samples_split\": [2, 4, 8],\n",
        "        \"min_samples_leaf\": [1, 2, 4],\n",
        "        \"bootstrap\": [True, False]\n",
        "    }\n",
        "    rf_rs = run_random_search(RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS), rf_param_dist, X, y, name=\"random_forest\")\n",
        "\n",
        "    # -------- XGBoost tuning (if available) --------\n",
        "    if HAS_XGB:\n",
        "        xgb_param_dist = {\n",
        "            \"n_estimators\": [100,200,300,400],\n",
        "            \"learning_rate\": [0.01,0.03,0.05,0.1],\n",
        "            \"max_depth\": [3,4,5,6],\n",
        "            \"subsample\": [0.6,0.8,1.0],\n",
        "            \"colsample_bytree\": [0.6,0.8,1.0]\n",
        "        }\n",
        "        xgb_rs = run_random_search(XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RANDOM_STATE, n_jobs=1), xgb_param_dist, X, y, name=\"xgboost\")\n",
        "    else:\n",
        "        print(\"\\nXGBoost not installed — skipping XGB tuning.\")\n",
        "\n",
        "    # -------- LightGBM tuning (if available) --------\n",
        "    if HAS_LGB:\n",
        "        lgb_param_dist = {\n",
        "            \"n_estimators\": [100,200,300,400],\n",
        "            \"learning_rate\": [0.01,0.03,0.05,0.1],\n",
        "            \"num_leaves\": [15,31,63],\n",
        "            \"feature_fraction\": [0.6,0.8,1.0]\n",
        "        }\n",
        "        lgb_rs = run_random_search(LGBMClassifier(random_state=RANDOM_STATE, n_jobs=1), lgb_param_dist, X, y, name=\"lightgbm\")\n",
        "    else:\n",
        "        print(\"\\nLightGBM not installed — skipping LGB tuning.\")\n",
        "\n",
        "    print(\"\\nDone. Models saved in 'models/' directory.\")\n",
        "    print(\"Next: • Use best estimators from these RandomizedSearchCV objects in your stacking pipeline.\")\n",
        "    print(\"Tip: If run time is long, reduce N_ITER or n_estimators and try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}