{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu7OhazMu8qc6JCwxIB3gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-19/day19_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Gn-9RcvNsxCN",
        "outputId": "daf326d6-a50b-4f3e-a955-f66b04d395d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shapes — train: (712, 28) val: (179, 28)\n",
            "Training stacking pipeline...\n",
            "Validation Accuracy: 0.8212\n",
            "CV Accuracy: 0.8316 (+/- 0.0189)\n",
            "✅ Saved pipeline to: models/day19_stacking_20250926_1555_pipeline.pkl\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-884368723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-884368723.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# Permutation importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mpermutation_importance_on_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Predict test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-884368723.py\u001b[0m in \u001b[0;36mpermutation_importance_on_pipeline\u001b[0;34m(pipeline, X_val, y_val, out_prefix)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Create aligned DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     perm_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;34m\"importance_mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "# day19_titanic.py\n",
        "# -------------------------------------------------------\n",
        "# Titanic ML Challenge - Day 19 (Stacking + Explainability)\n",
        "# -------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Paths\n",
        "# ---------------------------\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "MODEL_DIR = Path(\"models\")\n",
        "REPORT_DIR = Path(\"reports\")\n",
        "\n",
        "for p in [MODEL_DIR, REPORT_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_path = DATA_DIR / \"train_processed.csv\"\n",
        "test_path = DATA_DIR / \"test_processed.csv\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Load data\n",
        "# ---------------------------\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "# 🔑 Fix: convert all bools → str upfront\n",
        "for df in [train, test]:\n",
        "    for col in df.select_dtypes(include=[\"bool\"]).columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "X = train.drop([\"Survived\", \"PassengerId\"], axis=1)\n",
        "y = train[\"Survived\"]\n",
        "\n",
        "X_test = test.drop([\"PassengerId\"], axis=1)\n",
        "test_passenger_id = test[\"PassengerId\"]\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Preprocessor\n",
        "# ---------------------------\n",
        "def build_preprocessor(X_df):\n",
        "    \"\"\"Build preprocessing pipeline for numerical + categorical features.\"\"\"\n",
        "\n",
        "    numeric_features = X_df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "    categorical_features = X_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "    num_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    cat_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", num_transformer, numeric_features),\n",
        "            (\"cat\", cat_transformer, categorical_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Models\n",
        "# ---------------------------\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=7, random_state=42\n",
        ")\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42\n",
        ")\n",
        "\n",
        "base_models = [\n",
        "    (\"rf\", rf),\n",
        "    (\"gb\", gb),\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Pipeline\n",
        "# ---------------------------\n",
        "def build_pipeline(X_df):\n",
        "    preprocessor = build_preprocessor(X_df)\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"stack\", stack_model)\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Permutation Importance\n",
        "# ---------------------------\n",
        "def permutation_importance_on_pipeline(pipeline, X_val, y_val, out_prefix=\"run\"):\n",
        "    result = permutation_importance(\n",
        "        pipeline, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Get feature names\n",
        "    preprocessor = pipeline.named_steps[\"preprocessor\"]\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    # Create aligned DataFrame\n",
        "    perm_df = pd.DataFrame({\n",
        "        \"feature\": feature_names,\n",
        "        \"importance_mean\": result.importances_mean,\n",
        "        \"importance_std\": result.importances_std\n",
        "    }).sort_values(by=\"importance_mean\", ascending=False)\n",
        "\n",
        "    out_path = REPORT_DIR / f\"{out_prefix}_permutation_importance.csv\"\n",
        "    perm_df.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Permutation importance saved to {out_path}\")\n",
        "\n",
        "    return perm_df\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Main\n",
        "# ---------------------------\n",
        "def main():\n",
        "    prefix = f\"day19_stacking_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "\n",
        "    # Split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Data shapes — train: {X_train.shape} val: {X_val.shape}\")\n",
        "\n",
        "    pipeline = build_pipeline(X)\n",
        "\n",
        "    # Train\n",
        "    print(\"Training stacking pipeline...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_pred = pipeline.predict(X_val)\n",
        "    val_acc = accuracy_score(y_val, val_pred)\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # CV accuracy\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "    print(f\"CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Save pipeline\n",
        "    model_path = MODEL_DIR / f\"{prefix}_pipeline.pkl\"\n",
        "    joblib.dump(pipeline, model_path)\n",
        "    print(f\"✅ Saved pipeline to: {model_path}\")\n",
        "\n",
        "    # Permutation importance\n",
        "    permutation_importance_on_pipeline(pipeline, X_val, y_val, out_prefix=prefix)\n",
        "\n",
        "    # Predict test set\n",
        "    test_pred = pipeline.predict(X_test)\n",
        "    submission = pd.DataFrame({\n",
        "        \"PassengerId\": test_passenger_id,\n",
        "        \"Survived\": test_pred\n",
        "    })\n",
        "\n",
        "    out_path = Path(\"submissions\") / f\"{prefix}.csv\"\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    submission.to_csv(out_path, index=False)\n",
        "    print(f\"✅ Submission saved to {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}