{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI1YlYP/k5rb626e6QaJiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-6/day06_ensemblesipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs3sK1DgU0iW",
        "outputId": "8e10631f-84c7-466a-bbbf-3be6471930fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Day 2 cleaning script starting ===\n",
            "Found train file at: /content/train.csv\n",
            "Raw data shape: (891, 12)\n",
            "Saved cleaned (minimal) dataset -> day02/day02_titanic_clean.csv (shape: (891, 12))\n",
            "Saved preserved dataset -> day02/day02_titanic_preserved.csv (shape: (891, 15))\n",
            "\n",
            "--- Quick checks on cleaned file ---\n",
            "Columns: ['PassengerId', 'Survived', 'Pclass', 'Sex', 'Sex_bin', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']\n",
            "Missing values per column:\n",
            " PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Sex            0\n",
            "Sex_bin        0\n",
            "Age            0\n",
            "SibSp          0\n",
            "Parch          0\n",
            "Fare           0\n",
            "Embarked       0\n",
            "FamilySize     0\n",
            "IsAlone        0\n",
            "dtype: int64\n",
            "\n",
            "Sample rows:\n",
            "  PassengerId  Survived  Pclass    Sex  Sex_bin  Age  SibSp  Parch    Fare Embarked  FamilySize  IsAlone\n",
            "           1         0       3   male        1 22.0      1      0  7.2500        S           2        0\n",
            "           2         1       1 female        0 38.0      1      0 71.2833        C           2        0\n",
            "           3         1       3 female        0 26.0      0      0  7.9250        S           1        1\n",
            "           4         1       1 female        0 35.0      1      0 53.1000        S           2        0\n",
            "           5         0       3   male        1 35.0      0      0  8.0500        S           1        1\n",
            "\n",
            "If everything looks good, commit the files:\n",
            "  git add day02/*\n",
            "  git commit -m \"Day 2: Titanic cleaned dataset saved\"\n",
            "  git push\n",
            "=== Done ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-218996352.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
            "/tmp/ipython-input-218996352.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "/tmp/ipython-input-218996352.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(mode_emb, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# day06_ensembles.py\n",
        "# Run from your project root. Creates day06/assets/ with plots & saves models/results.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        "import joblib\n",
        "\n",
        "# Optional xgboost (try import)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "# -------------------------\n",
        "# I/O & folders\n",
        "# -------------------------\n",
        "OUT = Path(\"day06\")\n",
        "ASSETS = OUT / \"assets\"\n",
        "OUT.mkdir(exist_ok=True)\n",
        "ASSETS.mkdir(exist_ok=True)\n",
        "\n",
        "# Try to load best processed features first, fallback to earlier cleaned files\n",
        "processed_candidates = [Path(\"day05/day05_titanic_feat.csv\"), Path(\"day05_titanic_feat.csv\")]\n",
        "clean_candidates = [Path(\"day02/day02_titanic_clean.csv\"), Path(\"day02/day02_titanic_preserved.csv\"), Path(\"day02/train.csv\"), Path(\"train.csv\")]\n",
        "\n",
        "data = None\n",
        "for p in processed_candidates:\n",
        "    if p.exists():\n",
        "        print(\"Using processed features:\", p)\n",
        "        data = pd.read_csv(p)\n",
        "        break\n",
        "\n",
        "if data is None:\n",
        "    # fallback to cleaned file and do quick preprocessing (safe)\n",
        "    for p in clean_candidates:\n",
        "        if p.exists():\n",
        "            print(\"Loading cleaned/raw file:\", p)\n",
        "            raw = pd.read_csv(p)\n",
        "            data = raw.copy()\n",
        "            break\n",
        "\n",
        "if data is None:\n",
        "    raise FileNotFoundError(\"No data found. Place 'day05/day05_titanic_feat.csv' or 'day02/day02_titanic_clean.csv' or upload 'train.csv' in project root.\")\n",
        "\n",
        "# If loaded processed file already includes target 'Survived'\n",
        "if 'Survived' not in data.columns:\n",
        "    # try to find Survived in candidate raw set - if not found, error\n",
        "    raise KeyError(\"Loaded dataset does not contain 'Survived' column. Ensure you pass processed file with target.\")\n",
        "\n",
        "# -------------------------\n",
        "# Prepare X, y\n",
        "# -------------------------\n",
        "# If processed (many numeric columns), use as is; else do safe minimal transforms\n",
        "def prepare_features(df):\n",
        "    df = df.copy()\n",
        "    # drop obvious unused columns if present\n",
        "    drop_cols = ['PassengerId','Ticket','Cabin','Name']\n",
        "    for c in drop_cols:\n",
        "        if c in df.columns:\n",
        "            df.drop(columns=[c], inplace=True)\n",
        "    # If Sex is present as string, convert to binary\n",
        "    if 'Sex' in df.columns and df['Sex'].dtype == object:\n",
        "        df['Sex'] = (df['Sex'].str.lower().str.startswith('m')).astype(int)\n",
        "    # Fill Age/Fare missing if any\n",
        "    if 'Age' in df.columns:\n",
        "        df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
        "        df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "    if 'Fare' in df.columns:\n",
        "        df['Fare'] = pd.to_numeric(df['Fare'], errors='coerce')\n",
        "        df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
        "    # One-hot encode remaining categorical (safe)\n",
        "    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
        "    cat_cols = [c for c in cat_cols if c != 'Survived']\n",
        "    if len(cat_cols) > 0:\n",
        "        df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "    return df\n",
        "\n",
        "df = prepare_features(data)\n",
        "print(\"Prepared data shape:\", df.shape)\n",
        "\n",
        "# Separate X,y\n",
        "y = df['Survived']\n",
        "X = df.drop(columns=['Survived'])\n",
        "\n",
        "# Train/test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
        "print(\"Train/Test:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: evaluate & save metrics\n",
        "# -------------------------\n",
        "def evaluate_model(name, model, X_test, y_test, save_prefix):\n",
        "    y_pred = model.predict(X_test)\n",
        "    proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
        "        \"precision\": float(precision_score(y_test, y_pred)),\n",
        "        \"recall\": float(recall_score(y_test, y_pred)),\n",
        "        \"f1\": float(f1_score(y_test, y_pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_test, proba)) if proba is not None else None\n",
        "    }\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"metrics:\", metrics)\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ASSETS / f\"{save_prefix}_confusion.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # ROC plot (if proba)\n",
        "    if proba is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, proba)\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={metrics['roc_auc']:.3f})\")\n",
        "        plt.plot([0,1], [0,1], '--', color='grey')\n",
        "        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"{name} ROC\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(ASSETS / f\"{save_prefix}_roc.png\")\n",
        "        plt.close()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "results = {}\n",
        "\n",
        "# -------------------------\n",
        "# 1) Random Forest baseline\n",
        "# -------------------------\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "results['random_forest'] = evaluate_model(\"RandomForest\", rf, X_test, y_test, \"rf\")\n",
        "\n",
        "# Save RF model\n",
        "joblib.dump(rf, ASSETS / \"rf_model.joblib\")\n",
        "\n",
        "# Feature importances (top 20)\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "imp_top = importances.head(20)\n",
        "plt.figure(figsize=(6,8))\n",
        "sns.barplot(x=imp_top.values, y=imp_top.index)\n",
        "plt.title(\"RF Top Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(ASSETS / \"rf_feature_importances.png\")\n",
        "plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Gradient Boosting (sklearn)\n",
        "# -------------------------\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "results['gradient_boosting'] = evaluate_model(\"GradientBoosting\", gb, X_test, y_test, \"gb\")\n",
        "joblib.dump(gb, ASSETS / \"gb_model.joblib\")\n",
        "\n",
        "# -------------------------\n",
        "# 3) XGBoost (if available)\n",
        "# -------------------------\n",
        "if XGBOOST_AVAILABLE:\n",
        "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    results['xgboost'] = evaluate_model(\"XGBoost\", xgb, X_test, y_test, \"xgb\")\n",
        "    joblib.dump(xgb, ASSETS / \"xgb_model.joblib\")\n",
        "else:\n",
        "    print(\"XGBoost not installed. To enable, run: pip install xgboost\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Stacking ensemble (use RF + GB + optionally XGB)\n",
        "# -------------------------\n",
        "estimators = [('rf', rf), ('gb', gb)]\n",
        "if XGBOOST_AVAILABLE:\n",
        "    estimators.append(('xgb', xgb))\n",
        "\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000), n_jobs=-1)\n",
        "stack.fit(X_train, y_train)\n",
        "results['stacking'] = evaluate_model(\"Stacking\", stack, X_test, y_test, \"stack\")\n",
        "joblib.dump(stack, ASSETS / \"stack_model.joblib\")\n",
        "\n",
        "# -------------------------\n",
        "# 5) Summarize & save results\n",
        "# -------------------------\n",
        "with open(OUT / \"day06_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\nAll done. Results saved to:\", OUT)\n",
        "print(\"Assets (plots & models) saved to:\", ASSETS)\n",
        "print(\"Quick look at results:\", results)\n"
      ]
    }
  ]
}
