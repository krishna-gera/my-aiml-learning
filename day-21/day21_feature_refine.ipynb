{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVbFhfvf+XTcEdfUyE1TX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-21/day21_feature_refine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3618Rk815kid"
      },
      "outputs": [],
      "source": [
        "# day21_feature_refine.py\n",
        "\"\"\"\n",
        "Day 21 - Feature refinement (2-hour focused script)\n",
        "- Uses previous SHAP / permutation importance if available\n",
        "- Creates a small set of interaction features from top K features\n",
        "- Compares baseline CV vs refined CV with RandomForest\n",
        "- Saves artifacts to outputs/ and models/\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "MODEL_DIR = Path(\"models\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = DATA_DIR / \"train_processed.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test_processed.csv\"   # optional\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 5\n",
        "TOP_K = 10           # how many top features to consider for interactions\n",
        "MAX_INTERACTIONS = 15  # limit pairwise interactions created\n",
        "RF_ESTIMATORS = 150  # keep small for speed; increase later\n",
        "# ----------------------------\n",
        "\n",
        "def load_data():\n",
        "    train = pd.read_csv(TRAIN_CSV)\n",
        "    test = pd.read_csv(TEST_CSV) if TEST_CSV.exists() else None\n",
        "\n",
        "    # Safety: convert bools -> str to avoid imputer dtype issues\n",
        "    for df in [train, test] if test is not None else [train]:\n",
        "        bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
        "        for c in bool_cols:\n",
        "            df[c] = df[c].astype(str)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def encode_and_impute(train_df, test_df=None):\n",
        "    \"\"\"One-hot encode via pandas.get_dummies and median-impute numeric NaNs.\n",
        "       Returns (X_train_enc_df, X_test_enc_df or None, imputer).\"\"\"\n",
        "    # drop Survived if present in train_df\n",
        "    X_train = train_df.copy()\n",
        "    if \"Survived\" in X_train.columns:\n",
        "        X_train = X_train.drop(columns=[\"Survived\"])\n",
        "    # use get_dummies for categorical encoding (safe)\n",
        "    X_train_enc = pd.get_dummies(X_train, dummy_na=False)\n",
        "\n",
        "    X_test_enc = None\n",
        "    if test_df is not None:\n",
        "        X_test = test_df.copy()\n",
        "        X_test_enc = pd.get_dummies(X_test, dummy_na=False)\n",
        "        # align columns\n",
        "        X_train_enc, X_test_enc = X_train_enc.align(X_test_enc, join=\"left\", axis=1, fill_value=0)\n",
        "\n",
        "    # Impute medians\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_enc), columns=X_train_enc.columns)\n",
        "    if X_test_enc is not None:\n",
        "        X_test_imp = pd.DataFrame(imputer.transform(X_test_enc), columns=X_test_enc.columns)\n",
        "    else:\n",
        "        X_test_imp = None\n",
        "\n",
        "    return X_train_imp, X_test_imp, imputer\n",
        "\n",
        "def cv_score_rf(X, y, n_estimators=RF_ESTIMATORS):\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_val_score(rf, X, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n",
        "    return float(scores.mean()), float(scores.std()), scores\n",
        "\n",
        "def find_latest_importance():\n",
        "    \"\"\"Look for latest SHAP or permutation importance CSV in outputs/\"\"\"\n",
        "    patterns = [\"outputs/*shap_importance*.csv\", \"outputs/*permutation_importance*.csv\", \"reports/*permutation_importance*.csv\", \"reports/*shap_importance*.csv\"]\n",
        "    files = []\n",
        "    for p in patterns:\n",
        "        files.extend(glob.glob(p))\n",
        "    files = sorted(files)\n",
        "    return files[-1] if files else None\n",
        "\n",
        "def quick_feature_importance_from_rf(X, y):\n",
        "    \"\"\"Fit a quick RF and return feature importances (DataFrame).\"\"\"\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    rf.fit(X, y)\n",
        "    imp = pd.Series(rf.feature_importances_, index=X.columns).sort_va_\n"
      ]
    }
  ]
}