{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRY8gwnuv6NvgTH8cM8xcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-28/day28_capstone_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vIzM8sBUJAl",
        "outputId": "4c1530cc-f281-4c15-817f-fcb300d5e6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Outputs:\n",
            "- day28_submission.csv\n",
            "- day28_models.joblib\n",
            "- day28_report.json\n"
          ]
        }
      ],
      "source": [
        "# day28_stacking_final.py\n",
        "\"\"\"\n",
        "Day 28 - Stacking ensemble, advanced threshold tuning, final submission.\n",
        "Inputs:\n",
        "  - train_processed.csv  (must contain TARGET column, e.g. 'Survived')\n",
        "  - test_processed.csv   (must contain PassengerId/Id)\n",
        "Outputs:\n",
        "  - day28_submission.csv\n",
        "  - day28_models.joblib\n",
        "  - day28_report.json\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---------------------------\n",
        "# Config / Filenames\n",
        "# ---------------------------\n",
        "TRAIN_FILE = \"train_processed.csv\"\n",
        "TEST_FILE = \"test_processed.csv\"\n",
        "TARGET = \"Survived\"\n",
        "ID_CANDIDATES = [\"PassengerId\", \"Id\", \"ID\", \"passengerid\"]\n",
        "\n",
        "OUTPUT_MODELS = \"day28_models.joblib\"\n",
        "OUTPUT_SUB = \"day28_submission.csv\"\n",
        "OUTPUT_REPORT = \"day28_report.json\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "def find_id_col(df):\n",
        "    for c in ID_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def version_safe_ohe():\n",
        "    from packaging import version\n",
        "    import sklearn\n",
        "    if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    else:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def build_preprocessor(X_df):\n",
        "    numeric_cols = X_df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "    categorical_cols = X_df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "    num_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scale\", StandardScaler())\n",
        "    ])\n",
        "    transformers = [(\"num\", num_pipe, numeric_cols)]\n",
        "    if categorical_cols:\n",
        "        cat_pipe = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", version_safe_ohe())\n",
        "        ])\n",
        "        transformers.append((\"cat\", cat_pipe, categorical_cols))\n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "    return preprocessor, numeric_cols, categorical_cols\n",
        "\n",
        "def apply_rule_to_preds(preds, X_df, rule):\n",
        "    \"\"\"Apply single numeric/categorical rule to predictions.\"\"\"\n",
        "    preds = preds.copy()\n",
        "    if rule is None:\n",
        "        return preds\n",
        "    if rule[\"type\"] == \"numeric\":\n",
        "        feat = rule[\"feature\"]\n",
        "        thr = float(rule[\"threshold\"])\n",
        "        if rule[\"op\"] == \">=\":\n",
        "            cond = (X_df[feat].values >= thr) & (preds == 0)\n",
        "            preds[cond] = 1\n",
        "        elif rule[\"op\"] == \"<=\":\n",
        "            cond = (X_df[feat].values <= thr) & (preds == 1)\n",
        "            preds[cond] = 0\n",
        "    elif rule[\"type\"] == \"categorical\":\n",
        "        feat = rule[\"feature\"]\n",
        "        val = rule[\"value\"]\n",
        "        if rule[\"op\"] == \"==\":\n",
        "            cond = (X_df[feat].values == val) & (preds == 0)\n",
        "            preds[cond] = 1\n",
        "    return preds\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "def main():\n",
        "    # 1) Load data\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        raise FileNotFoundError(\"Make sure train_processed.csv and test_processed.csv are in the working directory.\")\n",
        "\n",
        "    train = pd.read_csv(TRAIN_FILE)\n",
        "    test = pd.read_csv(TEST_FILE)\n",
        "    id_col = find_id_col(test)\n",
        "\n",
        "    if TARGET not in train.columns:\n",
        "        raise ValueError(f\"Target column '{TARGET}' not found in train file.\")\n",
        "\n",
        "    drop_cols = [TARGET]\n",
        "    if id_col and id_col in train.columns:\n",
        "        drop_cols.append(id_col)\n",
        "\n",
        "    X = train.drop(columns=drop_cols, errors=\"ignore\")\n",
        "    y = train[TARGET].copy()\n",
        "    X_test = test.drop(columns=[id_col], errors=\"ignore\") if id_col else test.copy()\n",
        "    test_ids = test[id_col] if id_col else pd.Series(np.arange(len(test)), name=\"Id\")\n",
        "\n",
        "    # 2) Split train/holdout\n",
        "    X_train, X_hold, y_train, y_hold = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    # 3) Preprocessing\n",
        "    preprocessor, numeric_cols, categorical_cols = build_preprocessor(X_train)\n",
        "    X_train_proc = preprocessor.fit_transform(X_train)\n",
        "    X_hold_proc = preprocessor.transform(X_hold)\n",
        "    X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "    # 4) Define base models\n",
        "    rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    gb = GradientBoostingClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=600, random_state=RANDOM_STATE)\n",
        "\n",
        "    # 5) Stacking ensemble\n",
        "    stack_clf = StackingClassifier(\n",
        "        estimators=[('rf', rf), ('gb', gb), ('mlp', mlp)],\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        passthrough=True\n",
        "    )\n",
        "    stack_clf.fit(X_train_proc, y_train)\n",
        "\n",
        "    # 6) Evaluate holdout\n",
        "    hold_proba = stack_clf.predict_proba(X_hold_proc)[:,1]\n",
        "    hold_pred = (hold_proba >= 0.5).astype(int)\n",
        "    base_f1 = f1_score(y_hold, hold_pred)\n",
        "    base_acc = accuracy_score(y_hold, hold_pred)\n",
        "    base_roc = roc_auc_score(y_hold, hold_proba)\n",
        "\n",
        "    # 7) Threshold tuning (maximize F1)\n",
        "    best_t = 0.5\n",
        "    best_f1 = base_f1\n",
        "    for t in np.linspace(0.1, 0.9, 81):\n",
        "        pred_t = (hold_proba >= t).astype(int)\n",
        "        cur_f1 = f1_score(y_hold, pred_t)\n",
        "        if cur_f1 > best_f1:\n",
        "            best_f1 = cur_f1\n",
        "            best_t = float(t)\n",
        "    # Apply tuned threshold\n",
        "    hold_pred = (hold_proba >= best_t).astype(int)\n",
        "\n",
        "    # 8) Optional: simple rule (reuse Day27 selected_rule if exists)\n",
        "    selected_rule = None\n",
        "    if os.path.exists(\"day27_models.joblib\"):\n",
        "        saved27 = joblib.load(\"day27_models.joblib\")\n",
        "        if \"selected_rule\" in saved27:\n",
        "            selected_rule = saved27[\"selected_rule\"]\n",
        "            hold_pred = apply_rule_to_preds(hold_pred, X_hold, selected_rule)\n",
        "\n",
        "    # 9) Final test predictions\n",
        "    test_proba = stack_clf.predict_proba(X_test_proc)[:,1]\n",
        "    test_pred = (test_proba >= best_t).astype(int)\n",
        "    if selected_rule:\n",
        "        test_pred = apply_rule_to_preds(test_pred, X_test, selected_rule)\n",
        "\n",
        "    # 10) Save submission, models, report\n",
        "    submission = pd.DataFrame({id_col if id_col else \"Id\": test_ids, TARGET: test_pred})\n",
        "    submission.to_csv(OUTPUT_SUB, index=False)\n",
        "\n",
        "    saved_obj = {\n",
        "        \"preprocessor\": preprocessor,\n",
        "        \"stacking_model\": stack_clf,\n",
        "        \"best_threshold\": best_t,\n",
        "        \"selected_rule\": selected_rule\n",
        "    }\n",
        "    joblib.dump(saved_obj, OUTPUT_MODELS)\n",
        "\n",
        "    report = {\n",
        "        \"holdout_f1\": float(best_f1),\n",
        "        \"holdout_acc\": float(base_acc),\n",
        "        \"holdout_roc\": float(base_roc),\n",
        "        \"best_threshold\": best_t,\n",
        "        \"selected_rule\": selected_rule\n",
        "    }\n",
        "    with open(OUTPUT_REPORT, \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(\"Done. Outputs:\")\n",
        "    print(\"-\", OUTPUT_SUB)\n",
        "    print(\"-\", OUTPUT_MODELS)\n",
        "    print(\"-\", OUTPUT_REPORT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load saved Day28 stacking model\n",
        "saved = joblib.load(\"day28_models.joblib\")\n",
        "stack_model = saved[\"stacking_model\"]\n",
        "preprocessor = saved[\"preprocessor\"]\n",
        "best_t = saved[\"best_threshold\"]\n",
        "selected_rule = saved.get(\"selected_rule\", None)\n",
        "\n",
        "# Example: preprocess input DataFrame\n",
        "def predict_input(df_input):\n",
        "    X_proc = preprocessor.transform(df_input)\n",
        "    proba = stack_model.predict_proba(X_proc)[:,1]\n",
        "    preds = (proba >= best_t).astype(int)\n",
        "    if selected_rule:\n",
        "        preds = apply_rule_to_preds(preds, df_input, selected_rule)\n",
        "    return preds, proba\n"
      ],
      "metadata": {
        "id": "hGbv-BRaerTB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Define input features (use your X columns)\n",
        "feature_cols = list(preprocessor.feature_names_in_)  # all features used\n",
        "\n",
        "def predict_gradio(*args):\n",
        "    # create DataFrame from inputs\n",
        "    df_input = pd.DataFrame([args], columns=feature_cols)\n",
        "    preds, proba = predict_input(df_input)\n",
        "    return {\"Predicted Class\": int(preds[0]), \"Probability of 1\": float(proba[0])}\n",
        "\n",
        "# Build Gradio interface\n",
        "inputs = [gr.Number(label=col) for col in feature_cols]  # adjust types if categorical\n",
        "outputs = [gr.Label(num_top_classes=2)]\n",
        "\n",
        "iface = gr.Interface(fn=predict_gradio, inputs=inputs, outputs=outputs, live=True)\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "oIn5Ucm5etVp",
        "outputId": "8df45604-7e02-41a3-f890-805c0bd10019"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://340dadc44b61f27565.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://340dadc44b61f27565.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}