{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtPOqqALVoxZs2G9ZOCMiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-10/day10_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eUEMGiU9Fe2",
        "outputId": "b2f3d33a-8e90-4862-ccdd-b23bd1544cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: day02/day02_titanic_clean.csv\n",
            "Train rows: 757 Holdout rows: 134\n",
            "Saved processed features to: day10/day10_titanic_feat.csv\n",
            "Saved preprocessor to: day10/assets/preprocessor.joblib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3108978744.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Fare'].fillna(df['Fare'].median(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# day10_feature_engineering.py\n",
        "# Run from project root: python day10/day10_feature_engineering.py\n",
        "# Outputs: day10/day10_titanic_feat.csv, day10/assets/preprocessor.joblib, day10/assets/te_maps.json\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "OUT_DIR = Path(\"day10\"); OUT_DIR.mkdir(exist_ok=True)\n",
        "ASSETS = OUT_DIR / \"assets\"; ASSETS.mkdir(exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: safe load\n",
        "# -------------------------\n",
        "candidates = [\n",
        "    Path(\"day05/day05_titanic_feat.csv\"),\n",
        "    Path(\"day05_titanic_feat.csv\"),\n",
        "    Path(\"day02/day02_titanic_preserved.csv\"),\n",
        "    Path(\"day02/day02_titanic_clean.csv\"),\n",
        "    Path(\"train.csv\")\n",
        "]\n",
        "data_path = None\n",
        "for p in candidates:\n",
        "    if p.exists():\n",
        "        data_path = p\n",
        "        break\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError(\"No input CSV found. Place day05/day05_titanic_feat.csv or day02/day02_titanic_clean.csv or train.csv in project root.\")\n",
        "\n",
        "print(\"Loading:\", data_path)\n",
        "df_raw = pd.read_csv(data_path)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Basic feature engineering (create features in-place)\n",
        "# -------------------------\n",
        "df = df_raw.copy()\n",
        "\n",
        "# FamilySize and IsAlone\n",
        "if 'SibSp' in df.columns and 'Parch' in df.columns:\n",
        "    df['FamilySize'] = df['SibSp'].fillna(0).astype(int) + df['Parch'].fillna(0).astype(int) + 1\n",
        "else:\n",
        "    df['FamilySize'] = df.get('FamilySize', 1)\n",
        "\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
        "\n",
        "# Title from Name (if exists)\n",
        "if 'Name' in df.columns:\n",
        "    df['Title'] = df['Name'].str.extract(r',\\s*([^\\.]+)\\.').str.strip()\n",
        "    df['Title'] = df['Title'].replace({\n",
        "        \"Mlle\":\"Miss\",\"Ms\":\"Miss\",\"Mme\":\"Mrs\",\n",
        "        \"Countess\":\"Rare\",\"Lady\":\"Rare\",\"Sir\":\"Rare\",\"Don\":\"Rare\",\"Dona\":\"Rare\",\n",
        "        \"Col\":\"Rare\",\"Major\":\"Rare\",\"Capt\":\"Rare\",\"Rev\":\"Rare\",\"Dr\":\"Rare\",\"Jonkheer\":\"Rare\"\n",
        "    })\n",
        "    # Collapse very rare titles to 'Rare' (train later handles exact mapping)\n",
        "    vc = df['Title'].value_counts()\n",
        "    rare_titles = vc[vc < 10].index.tolist()\n",
        "    df.loc[df['Title'].isin(rare_titles), 'Title'] = 'Rare'\n",
        "else:\n",
        "    df['Title'] = 'Unknown'\n",
        "\n",
        "# Deck from Cabin (first letter), missing->'U'\n",
        "if 'Cabin' in df.columns:\n",
        "    df['Cabin'] = df['Cabin'].fillna('U')\n",
        "    df['Deck'] = df['Cabin'].astype(str).str[0]\n",
        "else:\n",
        "    df['Deck'] = 'U'\n",
        "\n",
        "# Fare per person\n",
        "if 'Fare' in df.columns:\n",
        "    df['Fare'] = pd.to_numeric(df['Fare'], errors='coerce')\n",
        "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
        "    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
        "    df['Fare_log'] = np.log1p(df['Fare'])\n",
        "else:\n",
        "    df['FarePerPerson'] = 0\n",
        "    df['Fare_log'] = 0\n",
        "\n",
        "# Age imputation by Title median (we will compute mapping from train only below)\n",
        "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
        "\n",
        "# Ticket group size (group by Ticket) - useful signal\n",
        "if 'Ticket' in df.columns:\n",
        "    ticket_counts = df['Ticket'].fillna('NA').map(df['Ticket'].value_counts())\n",
        "    df['TicketGroupSize'] = ticket_counts\n",
        "else:\n",
        "    df['TicketGroupSize'] = 1\n",
        "\n",
        "# Age bins\n",
        "df['AgeBin'] = pd.cut(df['Age'].fillna(df['Age'].median()), bins=[0,12,20,40,60,120],\n",
        "                      labels=['Child','Teen','Adult','MidAge','Senior'])\n",
        "\n",
        "# -------------------------\n",
        "# 2) Train/test split (so we can fit encoders on train only)\n",
        "# -------------------------\n",
        "if 'Survived' in df.columns:\n",
        "    X_full = df.drop(columns=[])\n",
        "    y_full = df['Survived']\n",
        "    X_train, X_hold, y_train, y_hold = train_test_split(X_full, y_full, test_size=0.15, stratify=y_full, random_state=42)\n",
        "    train_idx = X_train.index\n",
        "    print(\"Train rows:\", X_train.shape[0], \"Holdout rows:\", X_hold.shape[0])\n",
        "else:\n",
        "    # no label (rare) â€” we will treat all as train (useful if user already merged)\n",
        "    X_train = df.copy()\n",
        "    y_train = None\n",
        "    X_hold = None\n",
        "\n",
        "# -------------------------\n",
        "# 3) Age imputation by Title (compute mapping on TRAIN only)\n",
        "# -------------------------\n",
        "if 'Age' in X_train.columns:\n",
        "    age_map = X_train.groupby('Title')['Age'].median().to_dict()\n",
        "    # Fill train ages using title median where missing\n",
        "    X_train['Age'] = X_train.apply(lambda r: age_map[r['Title']] if pd.isna(r['Age']) and r['Title'] in age_map else r['Age'], axis=1)\n",
        "    # For holdout/test and full df, apply mapping with fallback to global median\n",
        "    global_age_med = X_train['Age'].median()\n",
        "    df['Age'] = df.apply(lambda r: age_map.get(r['Title'], global_age_med) if pd.isna(r['Age']) else r['Age'], axis=1)\n",
        "else:\n",
        "    age_map = {}\n",
        "    global_age_med = None\n",
        "\n",
        "# Save age map for reproducibility\n",
        "with open(ASSETS / \"age_map.json\", \"w\") as f:\n",
        "    json.dump(age_map, f)\n",
        "\n",
        "# -------------------------\n",
        "# 4) K-Fold Target Encoding (safe) for 'Title' (example)\n",
        "# -------------------------\n",
        "# Only run if we have labels in train\n",
        "def kfold_target_encode(train_series, target_series, test_series=None, n_splits=5, seed=42):\n",
        "    \"\"\"\n",
        "    Returns (train_encoded_series, test_encoded_series, mapping)\n",
        "    train_encoded_series: out-of-fold target-mean encoding for train\n",
        "    test_encoded_series: mapping applied to test (if provided) filled with global mean\n",
        "    mapping: full-train mapping of category -> mean\n",
        "    \"\"\"\n",
        "    tr = train_series.reset_index(drop=True)\n",
        "    y = target_series.reset_index(drop=True)\n",
        "    oof = pd.Series(index=tr.index, dtype=float)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for train_idx, val_idx in skf.split(np.zeros(len(tr)), y):\n",
        "        # compute mapping on train fold\n",
        "        fold_means = pd.DataFrame({'cat': tr.iloc[train_idx], 'y': y.iloc[train_idx]}).groupby('cat')['y'].mean()\n",
        "        # map to validation fold\n",
        "        oof.iloc[val_idx] = tr.iloc[val_idx].map(fold_means)\n",
        "    global_mean = y.mean()\n",
        "    oof.fillna(global_mean, inplace=True)\n",
        "    mapping_full = pd.DataFrame({'cat': tr, 'y': y}).groupby('cat')['y'].mean().to_dict()\n",
        "    if test_series is not None:\n",
        "        test_enc = test_series.map(mapping_full).fillna(global_mean)\n",
        "    else:\n",
        "        test_enc = None\n",
        "    return oof, test_enc, mapping_full\n",
        "\n",
        "# Apply to Title (if labels exist)\n",
        "te_maps = {}\n",
        "if 'Survived' in df.columns:\n",
        "    tr_series = X_train['Title'].astype(str)\n",
        "    y_tr = y_train.reset_index(drop=True)\n",
        "    # test_series: apply mapping to entire df Title col (for final processed CSV)\n",
        "    test_series = df['Title'].astype(str)\n",
        "    train_te, test_te, mapping_full = kfold_target_encode(tr_series, y_tr, test_series=test_series, n_splits=5)\n",
        "    # store encoded values back\n",
        "    # For train rows -> set encoded values\n",
        "    df.loc[X_train.index, 'Title_te'] = train_te.values\n",
        "    # For rest -> set test encoded (mapping from full train)\n",
        "    df.loc[~df.index.isin(X_train.index), 'Title_te'] = test_te.loc[~df.index.isin(X_train.index)].values\n",
        "    te_maps['Title'] = mapping_full\n",
        "else:\n",
        "    # no labels -> naive mean encoding from whole df (not ideal but unavoidable)\n",
        "    te_maps['Title'] = df.groupby('Title')['Survived'].mean().to_dict() if 'Survived' in df.columns else {}\n",
        "\n",
        "# Save target encoding maps\n",
        "with open(ASSETS / \"te_maps.json\", \"w\") as f:\n",
        "    json.dump({k: {str(cat): float(v) for cat, v in m.items()} for k, m in te_maps.items()}, f, indent=2)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Build ColumnTransformer pipeline (numerics + categorical OHE)\n",
        "# -------------------------\n",
        "# Choose columns\n",
        "num_cols = [c for c in ['Age','Fare','FarePerPerson','TicketGroupSize','FamilySize','Title_te'] if c in df.columns]\n",
        "cat_cols = [c for c in ['Pclass','Sex','Embarked','Deck','AgeBin'] if c in df.columns]\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scale', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_cols),\n",
        "    ('cat', cat_pipeline, cat_cols),\n",
        "], remainder='drop', verbose_feature_names_out=False)\n",
        "\n",
        "# Fit preprocessor on TRAIN (to avoid leakage)\n",
        "# If we have labels, fit on X_train rows; otherwise fit on full df\n",
        "if 'Survived' in df.columns:\n",
        "    preprocessor.fit(df.loc[X_train.index, num_cols + cat_cols])\n",
        "else:\n",
        "    preprocessor.fit(df[num_cols + cat_cols])\n",
        "\n",
        "# Transform entire df to get final numeric matrix\n",
        "X_proc = preprocessor.transform(df[num_cols + cat_cols])\n",
        "feature_names_num = num_cols\n",
        "try:\n",
        "    ohe_cols = preprocessor.named_transformers_['cat'].named_steps['ohe'].get_feature_names_out(cat_cols)\n",
        "    feature_names = list(feature_names_num) + list(ohe_cols)\n",
        "except Exception:\n",
        "    feature_names = feature_names_num\n",
        "\n",
        "X_proc_df = pd.DataFrame(X_proc, columns=feature_names, index=df.index)\n",
        "\n",
        "# Add target back if present\n",
        "if 'Survived' in df.columns:\n",
        "    out_df = pd.concat([X_proc_df, df['Survived'].reset_index(drop=True)], axis=1)\n",
        "else:\n",
        "    out_df = X_proc_df.copy()\n",
        "\n",
        "# -------------------------\n",
        "# 6) Save processed CSV & preprocessor\n",
        "# -------------------------\n",
        "out_path = OUT_DIR / \"day10_titanic_feat.csv\"\n",
        "out_df.to_csv(out_path, index=False)\n",
        "joblib.dump(preprocessor, ASSETS / \"preprocessor.joblib\")\n",
        "print(\"Saved processed features to:\", out_path)\n",
        "print(\"Saved preprocessor to:\", ASSETS / \"preprocessor.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "C9l-YPBu_X6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}