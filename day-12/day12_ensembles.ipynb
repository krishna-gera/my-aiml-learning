{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbSkIvDX/E1pYEdnHDPMXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-12/day12_ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2aJNCN2MGCb",
        "outputId": "75fc96ac-f413-48f0-83c4-9dac460d8682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: day10/day10_titanic_feat.csv\n",
            "Shapes -> (712, 20) (179, 20)\n",
            "\n",
            "Training BaggingClassifier (50 trees)...\n",
            "\n",
            "== Bagging ==\n",
            " {\n",
            "  \"accuracy\": 0.8100558659217877,\n",
            "  \"precision\": 0.7868852459016393,\n",
            "  \"recall\": 0.6956521739130435,\n",
            "  \"f1\": 0.7384615384615385,\n",
            "  \"roc_auc\": 0.820882740447958\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       110\n",
            "           1       0.79      0.70      0.74        69\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "\n",
            "Training AdaBoost (50 estimators, shallow trees)...\n",
            "\n",
            "== AdaBoost ==\n",
            " {\n",
            "  \"accuracy\": 0.8044692737430168,\n",
            "  \"precision\": 0.7297297297297297,\n",
            "  \"recall\": 0.782608695652174,\n",
            "  \"f1\": 0.7552447552447552,\n",
            "  \"roc_auc\": 0.82733860342556\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       110\n",
            "           1       0.73      0.78      0.76        69\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.79      0.80      0.80       179\n",
            "weighted avg       0.81      0.80      0.81       179\n",
            "\n",
            "\n",
            "Running small GridSearchCV for AdaBoost (quick)...\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "AdaBoost GridSearch saved to day12/day12_adaboost_gridsearch.csv\n",
            "\n",
            "== AdaBoost (tuned) ==\n",
            " {\n",
            "  \"accuracy\": 0.7932960893854749,\n",
            "  \"precision\": 0.7424242424242424,\n",
            "  \"recall\": 0.7101449275362319,\n",
            "  \"f1\": 0.725925925925926,\n",
            "  \"roc_auc\": 0.8196310935441371\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       110\n",
            "           1       0.74      0.71      0.73        69\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.78      0.78       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "\n",
            "Training GradientBoostingClassifier (sklearn)...\n",
            "\n",
            "== GradientBoosting ==\n",
            " {\n",
            "  \"accuracy\": 0.7932960893854749,\n",
            "  \"precision\": 0.7758620689655172,\n",
            "  \"recall\": 0.6521739130434783,\n",
            "  \"f1\": 0.7086614173228346,\n",
            "  \"roc_auc\": 0.824703557312253\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       110\n",
            "           1       0.78      0.65      0.71        69\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.79      0.77      0.77       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "\n",
            "Training XGBoost (light config)...\n",
            "\n",
            "== XGBoost ==\n",
            " {\n",
            "  \"accuracy\": 0.7932960893854749,\n",
            "  \"precision\": 0.7758620689655172,\n",
            "  \"recall\": 0.6521739130434783,\n",
            "  \"f1\": 0.7086614173228346,\n",
            "  \"roc_auc\": 0.8115283267457182\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       110\n",
            "           1       0.78      0.65      0.71        69\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.79      0.77      0.77       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "\n",
            "Training VotingClassifier (LR + RF + AdaBoost/XGB if available)...\n",
            "\n",
            "== VotingClassifier ==\n",
            " {\n",
            "  \"accuracy\": 0.8100558659217877,\n",
            "  \"precision\": 0.8070175438596491,\n",
            "  \"recall\": 0.6666666666666666,\n",
            "  \"f1\": 0.7301587301587301,\n",
            "  \"roc_auc\": 0.8432147562582344\n",
            "}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       110\n",
            "           1       0.81      0.67      0.73        69\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.78      0.79       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "\n",
            "Saved summary to day12/day12_results.csv and day12/day12_results.json\n",
            "Assets saved to day12/assets\n",
            "\n",
            "Day 12 complete âœ…\n"
          ]
        }
      ],
      "source": [
        "# day12/day12_ensembles.py\n",
        "# Day 12: Bagging, Boosting, Voting - Titanic\n",
        "# Run from project root: python day12/day12_ensembles.py\n",
        "\n",
        "import warnings, json\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# optional xgboost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGB_OK = True\n",
        "except Exception:\n",
        "    XGB_OK = False\n",
        "\n",
        "OUT = Path(\"day12\"); OUT.mkdir(exist_ok=True)\n",
        "ASSETS = OUT / \"assets\"; ASSETS.mkdir(exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# 0) Load best processed dataset (prefer Day10 output)\n",
        "# --------------------------\n",
        "candidates = [\n",
        "    Path(\"day10/day10_titanic_feat.csv\"),\n",
        "    Path(\"day05/day05_titanic_feat.csv\"),\n",
        "    Path(\"day02/day02_titanic_clean.csv\"),\n",
        "    Path(\"train.csv\")\n",
        "]\n",
        "df = None\n",
        "for p in candidates:\n",
        "    if p.exists():\n",
        "        print(\"Loading:\", p)\n",
        "        df = pd.read_csv(p)\n",
        "        break\n",
        "if df is None:\n",
        "    raise FileNotFoundError(\"No input CSV found. Put processed file in day10/day10_titanic_feat.csv or similar.\")\n",
        "\n",
        "if 'Survived' not in df.columns:\n",
        "    raise KeyError(\"Loaded dataset must include 'Survived' column.\")\n",
        "\n",
        "# --------------------------\n",
        "# 1) Safe preprocessing (minimal)\n",
        "# --------------------------\n",
        "data = df.copy()\n",
        "# drop meta columns if present\n",
        "for c in ['PassengerId','Ticket','Cabin','Name']:\n",
        "    if c in data.columns: data.drop(columns=[c], inplace=True)\n",
        "\n",
        "# encode sex string -> binary if needed\n",
        "if 'Sex' in data.columns and data['Sex'].dtype == object:\n",
        "    data['Sex'] = (data['Sex'].str.lower().str.startswith('m')).astype(int)\n",
        "\n",
        "# numeric fill\n",
        "for col in ['Age','Fare']:\n",
        "    if col in data.columns:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "        data[col].fillna(data[col].median(), inplace=True)\n",
        "\n",
        "# one-hot encode any remaining object columns (safe)\n",
        "cat_cols = data.select_dtypes(include=['object','category']).columns.tolist()\n",
        "cat_cols = [c for c in cat_cols if c != 'Survived']\n",
        "if cat_cols:\n",
        "    data = pd.get_dummies(data, columns=cat_cols, drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['Survived'])\n",
        "y = data['Survived']\n",
        "\n",
        "# train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
        "print(\"Shapes ->\", X_train.shape, X_test.shape)\n",
        "\n",
        "# helper: evaluate & save plots\n",
        "def evaluate(name, model, X_test, y_test, prefix):\n",
        "    y_pred = model.predict(X_test)\n",
        "    prob = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
        "        \"precision\": float(precision_score(y_test, y_pred)),\n",
        "        \"recall\": float(recall_score(y_test, y_pred)),\n",
        "        \"f1\": float(f1_score(y_test, y_pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_test, prob)) if prob is not None else None\n",
        "    }\n",
        "    print(f\"\\n== {name} ==\\n\", json.dumps(metrics, indent=2))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # confusion\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(4,3)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues'); plt.title(f\"{name} Confusion\"); plt.xlabel(\"Pred\"); plt.ylabel(\"Actual\"); plt.tight_layout()\n",
        "    plt.savefig(ASSETS / f\"{prefix}_confusion.png\"); plt.close()\n",
        "\n",
        "    # ROC\n",
        "    if prob is not None:\n",
        "        fpr,tpr,_ = roc_curve(y_test, prob)\n",
        "        plt.figure(figsize=(5,4)); plt.plot(fpr,tpr,label=f\"AUC={metrics['roc_auc']:.3f}\"); plt.plot([0,1],[0,1],'--',color='gray'); plt.title(f\"{name} ROC\"); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(ASSETS / f\"{prefix}_roc.png\"); plt.close()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "results = {}\n",
        "\n",
        "# --------------------------\n",
        "# 2) Bagging (BaggingClassifier with DecisionTree)\n",
        "# --------------------------\n",
        "print(\"\\nTraining BaggingClassifier (50 trees)...\")\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=None),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "joblib.dump(bag, ASSETS / \"bagging.joblib\")\n",
        "results['bagging'] = evaluate(\"Bagging\", bag, X_test, y_test, \"bagging\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) AdaBoost (boosting)\n",
        "# --------------------------\n",
        "print(\"\\nTraining AdaBoost (50 estimators, shallow trees)...\")\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "ada.fit(X_train, y_train)\n",
        "joblib.dump(ada, ASSETS / \"adaboost.joblib\")\n",
        "results['adaboost'] = evaluate(\"AdaBoost\", ada, X_test, y_test, \"adaboost\")\n",
        "\n",
        "# Small GridSearch for AdaBoost (tiny grid, Day 5 style)\n",
        "print(\"\\nRunning small GridSearchCV for AdaBoost (quick)...\")\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100],\n",
        "    \"learning_rate\": [0.5, 1.0]\n",
        "}\n",
        "gs = GridSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42), param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
        "gs.fit(X_train, y_train)\n",
        "pd.DataFrame(gs.cv_results_).to_csv(OUT / \"day12_adaboost_gridsearch.csv\", index=False)\n",
        "print(\"AdaBoost GridSearch saved to day12/day12_adaboost_gridsearch.csv\")\n",
        "best_ada = gs.best_estimator_\n",
        "joblib.dump(best_ada, ASSETS / \"adaboost_tuned.joblib\")\n",
        "results['adaboost_grid'] = {\"best_params\": gs.best_params_, \"cv_best_score\": float(gs.best_score_)}\n",
        "results['adaboost_grid']['test_metrics'] = evaluate(\"AdaBoost (tuned)\", best_ada, X_test, y_test, \"adaboost_tuned\")\n",
        "\n",
        "# --------------------------\n",
        "# 4) GradientBoosting (sklearn)\n",
        "# --------------------------\n",
        "print(\"\\nTraining GradientBoostingClassifier (sklearn)...\")\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "joblib.dump(gb, ASSETS / \"gradient_boosting.joblib\")\n",
        "results['gradient_boosting'] = evaluate(\"GradientBoosting\", gb, X_test, y_test, \"gb\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) XGBoost (if available)\n",
        "# --------------------------\n",
        "if XGB_OK:\n",
        "    print(\"\\nTraining XGBoost (light config)...\")\n",
        "    xgb = XGBClassifier(n_estimators=150, learning_rate=0.05, max_depth=4, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    joblib.dump(xgb, ASSETS / \"xgboost.joblib\")\n",
        "    results['xgboost'] = evaluate(\"XGBoost\", xgb, X_test, y_test, \"xgb\")\n",
        "else:\n",
        "    print(\"\\nXGBoost not installed â€” skipping XGBoost step. To enable, pip install xgboost\")\n",
        "\n",
        "# --------------------------\n",
        "# 6) Voting Classifier (soft voting)\n",
        "# --------------------------\n",
        "print(\"\\nTraining VotingClassifier (LR + RF + AdaBoost/XGB if available)...\")\n",
        "estimators = [\n",
        "    ('lr', LogisticRegression(max_iter=2000)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
        "]\n",
        "if XGB_OK:\n",
        "    estimators.append(('xgb', XGBClassifier(n_estimators=100, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)))\n",
        "# use tuned ada if available\n",
        "if 'best_ada' in locals():\n",
        "    estimators.append(('ada', best_ada))\n",
        "else:\n",
        "    estimators.append(('ada', AdaBoostClassifier(n_estimators=50, estimator=DecisionTreeClassifier(max_depth=1), random_state=42)))\n",
        "\n",
        "voting = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
        "voting.fit(X_train, y_train)\n",
        "joblib.dump(voting, ASSETS / \"voting.joblib\")\n",
        "results['voting'] = evaluate(\"VotingClassifier\", voting, X_test, y_test, \"voting\")\n",
        "\n",
        "# --------------------------\n",
        "# 7) Compare & save summary table\n",
        "# --------------------------\n",
        "# Build a simple metrics table\n",
        "rows = []\n",
        "for k, v in results.items():\n",
        "    if isinstance(v, dict) and 'accuracy' in v:\n",
        "        rows.append({\"model\": k, **v})\n",
        "    elif isinstance(v, dict) and 'test_metrics' in v:\n",
        "        # for gridsearch case\n",
        "        tm = v['test_metrics']\n",
        "        rows.append({\"model\": k, **tm})\n",
        "df_res = pd.DataFrame(rows)\n",
        "df_res.to_csv(OUT / \"day12_results.csv\", index=False)\n",
        "with open(OUT / \"day12_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved summary to day12/day12_results.csv and day12/day12_results.json\")\n",
        "print(\"Assets saved to\", ASSETS)\n",
        "\n",
        "# --------------------------\n",
        "# 8) Quick observations file\n",
        "# --------------------------\n",
        "notes = [\n",
        "    \"Day 12: Bagging, Boosting, Voting experiments\",\n",
        "    f\"Top models by f1 (check day12_results.csv):\\n{df_res.sort_values('f1', ascending=False).to_string(index=False)}\",\n",
        "    \"Next steps: try stacking, larger RandomizedSearchCV on best booster, use SHAP for explanation.\"\n",
        "]\n",
        "with open(OUT / \"day12_notes.md\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(notes))\n",
        "\n",
        "print(\"\\nDay 12 complete âœ…\")"
      ]
    }
  ]
}