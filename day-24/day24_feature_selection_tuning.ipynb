{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4cwH8BdC3D4yIptfqWN2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-24/day24_feature_selection_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "trUTF5o68xsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f739bb6-b1be-4c6a-98e8-973fc8898768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'Fare_log', 'TicketGroupSize', 'Sex_male', 'Embarked_Q', 'Embarked_S', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T', 'Deck_U', 'AgeBin_Child', 'AgeBin_MidAge', 'AgeBin_Senior', 'AgeBin_Teen']\n",
            "Categorical features: []\n",
            "Starting RandomizedSearchCV... (this may take a while depending on n_iter)\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "\n",
            "Validation results:\n",
            "Accuracy: 0.8156\n",
            "ROC AUC: 0.8338\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.88      0.85       110\n",
            "         1.0       0.79      0.71      0.75        69\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.82      0.81       179\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[97 13]\n",
            " [20 49]]\n",
            "\n",
            "Computing permutation importances...\n",
            "\n",
            "Top features by permutation importance:\n",
            "        feature  importance_mean  importance_std\n",
            "       Title_Mr         0.167132        0.032612\n",
            "            Age         0.048883        0.010728\n",
            "         Pclass         0.042831        0.012140\n",
            "       Sex_male         0.024209        0.011479\n",
            "     Title_Rare         0.007914        0.002754\n",
            "TicketGroupSize         0.007914        0.006229\n",
            "      Title_Mrs         0.006052        0.005796\n",
            "           Fare         0.004655        0.007507\n",
            "         Deck_E         0.002328        0.004816\n",
            "  AgeBin_MidAge         0.001862        0.002634\n",
            "    AgeBin_Teen         0.001862        0.002634\n",
            "       Fare_log         0.001397        0.010728\n",
            "     Embarked_S         0.001397        0.006501\n",
            "     Title_Miss         0.001397        0.002419\n",
            "         Deck_D         0.000931        0.004465\n",
            "\n",
            "Saved best model to: day24_model.joblib\n",
            "Generating submission on test set...\n",
            "Saved submission to: day24_submission.csv\n",
            "Saved report to: day24_report.txt\n",
            "\n",
            "Done. Files generated:\n",
            "- day24_model.joblib\n",
            "- day24_submission.csv\n",
            "- day24_report.txt\n"
          ]
        }
      ],
      "source": [
        "# day24_feature_selection_tuning.py\n",
        "\"\"\"\n",
        "Day 24: Feature selection, hyperparameter tuning, explainability.\n",
        "Input: train_processed.csv (must have 'Survived' target) and test_processed.csv (has PassengerId or Id).\n",
        "Output: day24_submission.csv, day24_model.joblib, day24_report.txt\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# ---------------------------\n",
        "# Step 0: Files / Constants\n",
        "# ---------------------------\n",
        "TRAIN_FILE = \"train_processed.csv\"\n",
        "TEST_FILE  = \"test_processed.csv\"\n",
        "TARGET_COL = \"Survived\"   # from Day 23 workflow\n",
        "REPORT_FILE = \"day24_report.txt\"\n",
        "MODEL_FILE  = \"day24_model.joblib\"\n",
        "SUBMISSION_FILE = \"day24_submission.csv\"\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Load data\n",
        "# ---------------------------\n",
        "train = pd.read_csv(TRAIN_FILE)\n",
        "test  = pd.read_csv(TEST_FILE)\n",
        "\n",
        "# detect id column in test\n",
        "id_col = None\n",
        "for c in [\"PassengerId\", \"Id\", \"ID\", \"passengerid\"]:\n",
        "    if c in test.columns:\n",
        "        id_col = c\n",
        "        break\n",
        "\n",
        "if TARGET_COL not in train.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET_COL}' not found in {TRAIN_FILE}.\")\n",
        "\n",
        "# Separate features and target\n",
        "drop_cols = [TARGET_COL]\n",
        "if id_col and id_col in train.columns:\n",
        "    drop_cols.append(id_col)\n",
        "\n",
        "X = train.drop(columns=drop_cols, errors='ignore')\n",
        "y = train[TARGET_COL].copy()\n",
        "\n",
        "# Keep a copy of raw test ids\n",
        "if id_col:\n",
        "    test_ids = test[id_col].copy()\n",
        "    X_test_full = test.drop(columns=[id_col], errors='ignore')\n",
        "else:\n",
        "    test_ids = pd.Series(np.arange(len(test)), name=\"Id\")\n",
        "    X_test_full = test.copy()\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: detect numeric & categorical\n",
        "# ---------------------------\n",
        "# Separate boolean columns from other categorical features\n",
        "boolean_features = X.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "\n",
        "# Convert boolean features to integers and add to numeric features\n",
        "for col in boolean_features:\n",
        "    X[col] = X[col].astype(int)\n",
        "    X_test_full[col] = X_test_full[col].astype(int)\n",
        "numeric_features.extend(boolean_features)\n",
        "\n",
        "\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: preprocessing pipelines\n",
        "# ---------------------------\n",
        "numeric_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder=\"drop\"  # drop any unexpected columns\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: train/validation split\n",
        "# ---------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: pipeline + param distributions for RandomizedSearchCV\n",
        "# ---------------------------\n",
        "base_pipe = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# param distributions (list-of-dicts to search multiple estimators)\n",
        "param_distributions = [\n",
        "    {\n",
        "        \"classifier\": [RandomForestClassifier(random_state=RANDOM_STATE)],\n",
        "        \"classifier__n_estimators\": randint(100, 501),   # 100..500\n",
        "        \"classifier__max_depth\": [None, 5, 8, 12, 20],\n",
        "        \"classifier__min_samples_split\": randint(2, 11),\n",
        "        \"classifier__min_samples_leaf\": randint(1, 5),\n",
        "        \"classifier__max_features\": [\"sqrt\", \"log2\", None]\n",
        "    },\n",
        "    {\n",
        "        \"classifier\": [GradientBoostingClassifier(random_state=RANDOM_STATE)],\n",
        "        \"classifier__n_estimators\": randint(100, 501),\n",
        "        \"classifier__learning_rate\": uniform(0.01, 0.19),  # 0.01-0.2\n",
        "        \"classifier__max_depth\": randint(3, 9),\n",
        "        \"classifier__subsample\": [0.6, 0.8, 1.0]\n",
        "    },\n",
        "    {\n",
        "        \"classifier\": [LogisticRegression(random_state=RANDOM_STATE, max_iter=2000)],\n",
        "        \"classifier__C\": uniform(0.01, 10),\n",
        "        \"classifier__penalty\": [\"l2\"],\n",
        "        \"classifier__solver\": [\"lbfgs\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=base_pipe,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=30,                    # change if you want more/less exploration\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        "    cv=cv,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "print(\"Starting RandomizedSearchCV... (this may take a while depending on n_iter)\")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "best_score = search.best_score_\n",
        "\n",
        "# ---------------------------\n",
        "# Step 6: Evaluate on validation set\n",
        "# ---------------------------\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "if hasattr(best_model, \"predict_proba\"):\n",
        "    try:\n",
        "        y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
        "    except Exception:\n",
        "        y_val_proba = None\n",
        "else:\n",
        "    y_val_proba = None\n",
        "\n",
        "acc = accuracy_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_val_proba) if y_val_proba is not None else None\n",
        "clf_report = classification_report(y_val, y_val_pred)\n",
        "conf_mat = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "print(\"\\nValidation results:\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "if roc_auc is not None:\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", clf_report)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_mat)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 7: Permutation importance (explainability)\n",
        "# ---------------------------\n",
        "print(\"\\nComputing permutation importances...\")\n",
        "perm = permutation_importance(best_model, X_val, y_val, n_repeats=12, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "\n",
        "# Build post-preprocessing feature names:\n",
        "feature_names = []\n",
        "if numeric_features:\n",
        "    feature_names.extend(numeric_features)\n",
        "\n",
        "if categorical_features:\n",
        "    # get onehot names from the fitted OneHotEncoder\n",
        "    try:\n",
        "        ohe = best_model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        cat_ohe_names = list(ohe.get_feature_names_out(categorical_features))\n",
        "        feature_names.extend(cat_ohe_names)\n",
        "    except Exception:\n",
        "        # fallback: generic names\n",
        "        # Count produced columns:\n",
        "        n_cat_cols = perm.importances_mean.shape[0] - len(numeric_features)\n",
        "        cat_ohe_names = [f\"cat_ohe_{i}\" for i in range(n_cat_cols)]\n",
        "        feature_names.extend(cat_ohe_names)\n",
        "\n",
        "# If mismatch in length, just create generic names\n",
        "if len(feature_names) != perm.importances_mean.shape[0]:\n",
        "    feature_names = [f\"f_{i}\" for i in range(perm.importances_mean.shape[0])]\n",
        "\n",
        "feat_imp_df = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feature_names,\n",
        "        \"importance_mean\": perm.importances_mean,\n",
        "        \"importance_std\": perm.importances_std\n",
        "    })\n",
        "    .sort_values(\"importance_mean\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"\\nTop features by permutation importance:\")\n",
        "print(feat_imp_df.head(15).to_string(index=False))\n",
        "\n",
        "# ---------------------------\n",
        "# Step 8: Save model, submission, report\n",
        "# ---------------------------\n",
        "joblib.dump(best_model, MODEL_FILE)\n",
        "print(f\"\\nSaved best model to: {MODEL_FILE}\")\n",
        "\n",
        "# Create submission\n",
        "print(\"Generating submission on test set...\")\n",
        "test_preds = best_model.predict(X_test_full)\n",
        "submission = pd.DataFrame({id_col if id_col else \"Id\": test_ids, TARGET_COL: test_preds})\n",
        "submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "print(f\"Saved submission to: {SUBMISSION_FILE}\")\n",
        "\n",
        "# Save a short report\n",
        "report = {\n",
        "    \"best_params\": {k: str(v) for k, v in best_params.items()},\n",
        "    \"best_cv_score_roc_auc\": float(best_score) if hasattr(best_score, \"__float__\") else str(best_score),\n",
        "    \"val_accuracy\": float(acc),\n",
        "    \"val_roc_auc\": float(roc_auc) if roc_auc is not None else None,\n",
        "    \"classification_report\": clf_report,\n",
        "    \"top_features\": feat_imp_df.head(20).to_dict(orient=\"records\")\n",
        "}\n",
        "\n",
        "with open(REPORT_FILE, \"w\") as f:\n",
        "    f.write(json.dumps(report, indent=2))\n",
        "print(f\"Saved report to: {REPORT_FILE}\")\n",
        "\n",
        "print(\"\\nDone. Files generated:\")\n",
        "print(\"-\", MODEL_FILE)\n",
        "print(\"-\", SUBMISSION_FILE)\n",
        "print(\"-\", REPORT_FILE)"
      ]
    }
  ]
}