{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4ufNvL0jwZbq7Es20T4CP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-gera/my-aiml-learning/blob/main/day-27/day27_shap_rules_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ee9qKoZyMvp",
        "outputId": "05329c00-2281-489c-9861-88e8b797d007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Outputs:\n",
            "- day27_submission.csv\n",
            "- day27_models.joblib\n",
            "- day27_report.json\n"
          ]
        }
      ],
      "source": [
        "# day27_shap_rules_mlp.py\n",
        "\"\"\"\n",
        "Day 27 - SHAP explainability, simple rule discovery, MLP comparison, calibration & threshold tuning.\n",
        "Inputs:\n",
        "  - train_processed.csv  (must contain TARGET column, e.g. 'Survived')\n",
        "  - test_processed.csv   (must contain PassengerId/Id)\n",
        "Outputs:\n",
        "  - day27_submission.csv\n",
        "  - day27_models.joblib\n",
        "  - day27_report.json\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import sklearn\n",
        "from packaging import version\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Try to import shap (optional)\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "# ---------------------------\n",
        "# Config / Filenames\n",
        "# ---------------------------\n",
        "TRAIN_FILE = \"train_processed.csv\"\n",
        "TEST_FILE = \"test_processed.csv\"\n",
        "TARGET = \"Survived\"   # change if your target name differs\n",
        "ID_CANDIDATES = [\"PassengerId\", \"Id\", \"ID\", \"passengerid\"]\n",
        "\n",
        "OUTPUT_MODELS = \"day27_models.joblib\"\n",
        "OUTPUT_SUB = \"day27_submission.csv\"\n",
        "OUTPUT_REPORT = \"day27_report.json\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "SHAP_SAMPLE = 1000  # max samples for SHAP computations (keeps it fast)\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "def find_id_col(df):\n",
        "    for c in ID_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def version_safe_ohe():\n",
        "    if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    else:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def build_preprocessor(X_df):\n",
        "    numeric_cols = X_df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "    # Exclude 'bool' from categorical types passed to SimpleImputer\n",
        "    categorical_cols = X_df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "    num_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scale\", StandardScaler())\n",
        "    ])\n",
        "    transformers = [(\"num\", num_pipe, numeric_cols)]\n",
        "    if categorical_cols:\n",
        "        cat_pipe = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", version_safe_ohe())\n",
        "        ])\n",
        "        transformers.append((\"cat\", cat_pipe, categorical_cols))\n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "    return preprocessor, numeric_cols, categorical_cols\n",
        "\n",
        "def get_feature_names(preprocessor, numeric_cols, categorical_cols):\n",
        "    feature_names = []\n",
        "    if numeric_cols:\n",
        "        feature_names.extend(numeric_cols)\n",
        "    if categorical_cols:\n",
        "        try:\n",
        "            # get the fitted onehot encoder\n",
        "            ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "            ohe_names = list(ohe.get_feature_names_out(categorical_cols))\n",
        "            feature_names.extend(ohe_names)\n",
        "        except Exception:\n",
        "            # fallback generic names\n",
        "            n_extra = preprocessor.transformers_[0][2]  # not reliable but fallback below\n",
        "            # if fallback, create generic names while we know size from preprocessed shape later.\n",
        "            pass\n",
        "    return feature_names\n",
        "\n",
        "def cohen_d(a, b):\n",
        "    a = np.array(a, dtype=float)\n",
        "    b = np.array(b, dtype=float)\n",
        "    if len(a) < 2 or len(b) < 2:\n",
        "        return 0.0\n",
        "    na, nb = len(a), len(b)\n",
        "    s_a = a.std(ddof=1)\n",
        "    s_b = b.std(ddof=1)\n",
        "    pooled = np.sqrt(((na - 1)*s_a**2 + (nb - 1)*s_b**2) / (na + nb - 2)) if (na+nb-2)>0 else 0.0\n",
        "    if pooled == 0:\n",
        "        return 0.0\n",
        "    return (a.mean() - b.mean()) / pooled\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "def main():\n",
        "    # 1) load\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        raise FileNotFoundError(\"Make sure train_processed.csv and test_processed.csv are in the working directory.\")\n",
        "    train = pd.read_csv(TRAIN_FILE)\n",
        "    test = pd.read_csv(TEST_FILE)\n",
        "    id_col = find_id_col(test)\n",
        "\n",
        "    if TARGET not in train.columns:\n",
        "        raise ValueError(f\"Target column '{TARGET}' not found in {TRAIN_FILE}\")\n",
        "\n",
        "    drop_cols = [TARGET]\n",
        "    if id_col and id_col in train.columns:\n",
        "        drop_cols.append(id_col)\n",
        "\n",
        "    X = train.drop(columns=drop_cols, errors=\"ignore\")\n",
        "    y = train[TARGET].copy()\n",
        "\n",
        "    test_ids = test[id_col] if id_col else pd.Series(np.arange(len(test)), name=\"Id\")\n",
        "    X_test = test.drop(columns=[id_col], errors=\"ignore\") if id_col else test.copy()\n",
        "\n",
        "    # 2) split train/holdout\n",
        "    X_train, X_hold, y_train, y_hold = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    # 3) preprocessing\n",
        "    preprocessor, numeric_cols, categorical_cols = build_preprocessor(X_train)\n",
        "    X_train_proc = preprocessor.fit_transform(X_train)\n",
        "    X_hold_proc = preprocessor.transform(X_hold)\n",
        "    X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "    # feature names (best-effort)\n",
        "    feature_names = []\n",
        "    # numeric first\n",
        "    feature_names.extend(numeric_cols)\n",
        "    # categorical ohe names if possible\n",
        "    if categorical_cols:\n",
        "        try:\n",
        "            ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "            ohe_names = list(ohe.get_feature_names_out(categorical_cols))\n",
        "            feature_names.extend(ohe_names)\n",
        "        except Exception:\n",
        "            # create generic names based on transformed shape\n",
        "            total_cols = X_train_proc.shape[1]\n",
        "            if len(feature_names) < total_cols:\n",
        "                n_extra = total_cols - len(feature_names)\n",
        "                feature_names.extend([f\"cat_ohe_{i}\" for i in range(n_extra)])\n",
        "    else:\n",
        "        # ensure names length matches proc\n",
        "        total_cols = X_train_proc.shape[1]\n",
        "        if len(feature_names) < total_cols:\n",
        "            feature_names = [f\"f_{i}\" for i in range(total_cols)]\n",
        "\n",
        "    # 4) train models (RF and small MLP)\n",
        "    rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=600, random_state=RANDOM_STATE)\n",
        "\n",
        "    rf.fit(X_train_proc, y_train)\n",
        "    mlp.fit(X_train_proc, y_train)\n",
        "\n",
        "    # holdout predictions & metrics\n",
        "    rf_proba = rf.predict_proba(X_hold_proc)[:, 1]\n",
        "    rf_pred = (rf_proba >= 0.5).astype(int)\n",
        "    mlp_proba = mlp.predict_proba(X_hold_proc)[:, 1]\n",
        "    mlp_pred = (mlp_proba >= 0.5).astype(int)\n",
        "\n",
        "    def summarize(name, y_true, y_pred, y_proba):\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        roc = roc_auc_score(y_true, y_proba) if y_proba is not None else None\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        return {\"name\": name, \"accuracy\": float(acc), \"roc_auc\": float(roc) if roc is not None else None, \"f1\": float(f1)}\n",
        "\n",
        "    rf_stats = summarize(\"RandomForest\", y_hold, rf_pred, rf_proba)\n",
        "    mlp_stats = summarize(\"MLP\", y_hold, mlp_pred, mlp_proba)\n",
        "\n",
        "    # choose base for explainability & rule-search (the better ROC AUC on holdout)\n",
        "    base_model = rf if rf_stats[\"roc_auc\"] >= (mlp_stats[\"roc_auc\"] or 0) else mlp\n",
        "    base_proba = rf_proba if base_model is rf else mlp_proba\n",
        "    base_pred = rf_pred if base_model is rf else mlp_pred\n",
        "    chosen_name = \"RandomForest\" if base_model is rf else \"MLP\"\n",
        "\n",
        "    # 5) Explainability: SHAP if available, else permutation importance\n",
        "    shap_top_features = []\n",
        "    perm_top_features = []\n",
        "    try:\n",
        "        if SHAP_AVAILABLE and isinstance(base_model, RandomForestClassifier):\n",
        "            # compute TreeExplainer SHAP on a sample (fast)\n",
        "            expl = shap.TreeExplainer(base_model)\n",
        "            sample_idx = np.random.choice(X_train_proc.shape[0], min(SHAP_SAMPLE, X_train_proc.shape[0]), replace=False)\n",
        "            X_shap = X_train_proc[sample_idx]\n",
        "            shap_vals = expl.shap_values(X_shap)  # note: structure depends on shap version\n",
        "            # normalize to mean abs values\n",
        "            if isinstance(shap_vals, list) and len(shap_vals) >= 2:\n",
        "                # binary classifier -> shap_vals[1] corresponds to positive class\n",
        "                arr = np.abs(shap_vals[1]).mean(axis=0)\n",
        "            else:\n",
        "                arr = np.abs(shap_vals).mean(axis=0)\n",
        "            feat_imp = sorted(zip(feature_names, arr), key=lambda x: x[1], reverse=True)\n",
        "            shap_top_features = feat_imp[:20]\n",
        "        else:\n",
        "            raise Exception(\"SHAP not available or not a tree model -> fallback to permutation.\")\n",
        "    except Exception:\n",
        "        # permutation importance fallback (on holdout)\n",
        "        pi = permutation_importance(base_model, X_hold_proc, y_hold, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "        arr = pi.importances_mean\n",
        "        feat_imp = sorted(zip(feature_names, arr), key=lambda x: x[1], reverse=True)\n",
        "        perm_top_features = feat_imp[:20]\n",
        "\n",
        "    # 6) Error analysis on holdout (find FN/FP and numeric Cohen's d)\n",
        "    hold_df = X_hold.reset_index(drop=True).copy()\n",
        "    hold_df[TARGET] = y_hold.reset_index(drop=True)\n",
        "    hold_df[\"pred\"] = base_pred\n",
        "    hold_df[\"proba\"] = base_proba\n",
        "\n",
        "    tp = hold_df[(hold_df[TARGET] == 1) & (hold_df[\"pred\"] == 1)]\n",
        "    fn = hold_df[(hold_df[TARGET] == 1) & (hold_df[\"pred\"] == 0)]\n",
        "    tn = hold_df[(hold_df[TARGET] == 0) & (hold_df[\"pred\"] == 0)]\n",
        "    fp = hold_df[(hold_df[TARGET] == 0) & (hold_df[\"pred\"] == 1)]\n",
        "\n",
        "    numeric_candidates = numeric_cols.copy()\n",
        "    # compute Cohen's d for FN vs TP, FP vs TN\n",
        "    fn_diffs = []\n",
        "    fp_diffs = []\n",
        "    for col in numeric_candidates:\n",
        "        try:\n",
        "            d_fn = cohen_d(fn[col].dropna(), tp[col].dropna()) if (len(fn)>1 and len(tp)>1) else 0.0\n",
        "            d_fp = cohen_d(fp[col].dropna(), tn[col].dropna()) if (len(fp)>1 and len(tn)>1) else 0.0\n",
        "        except Exception:\n",
        "            d_fn, d_fp = 0.0, 0.0\n",
        "        fn_diffs.append((col, d_fn))\n",
        "        fp_diffs.append((col, d_fp))\n",
        "    fn_diffs_sorted = sorted(fn_diffs, key=lambda x: abs(x[1]), reverse=True)\n",
        "    fp_diffs_sorted = sorted(fp_diffs, key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    top_fn_cols = [c for c, _ in fn_diffs_sorted[:5]]\n",
        "    top_fp_cols = [c for c, _ in fp_diffs_sorted[:5]]\n",
        "    top_candidates = list(dict.fromkeys(top_fn_cols + top_fp_cols))  # unique preserve order\n",
        "\n",
        "    # 7) Candidate rule generation & evaluation (single-rule search)\n",
        "    # We'll search simple threshold rules for numeric features and category rules for categorical.\n",
        "    best_rule = {\"type\": None, \"feature\": None, \"op\": None, \"threshold\": None, \"improvement_f1\": 0.0, \"new_f1\": f1_score(y_hold, base_pred)}\n",
        "    base_f1 = f1_score(y_hold, base_pred)\n",
        "\n",
        "    # numeric rules: try thresholds at 25/50/75 percentiles\n",
        "    percentiles = [0.25, 0.5, 0.75]\n",
        "    for col in top_candidates:\n",
        "        if col not in hold_df.columns:\n",
        "            continue\n",
        "        col_vals = X_hold[col].dropna()\n",
        "        if len(col_vals) == 0:\n",
        "            continue\n",
        "        for p in percentiles:\n",
        "            thresh = float(np.nanpercentile(X_hold[col].values, p*100))\n",
        "            # two ops: if predicted==0 and val >= thresh -> flip to 1  (catch FN)\n",
        "            new_preds = base_pred.copy()\n",
        "            cond = (X_hold[col].values >= thresh) & (base_pred == 0)\n",
        "            new_preds[cond] = 1\n",
        "            new_f1 = f1_score(y_hold, new_preds)\n",
        "            if new_f1 > best_rule[\"new_f1\"]:\n",
        "                best_rule = {\"type\":\"numeric\", \"feature\":col, \"op\":\">=\", \"threshold\":thresh, \"improvement_f1\": new_f1 - base_f1, \"new_f1\": new_f1}\n",
        "            # other direction: if predicted==1 and val <= thresh -> flip to 0 (catch FP)\n",
        "            new_preds2 = base_pred.copy()\n",
        "            cond2 = (X_hold[col].values <= thresh) & (base_pred == 1)\n",
        "            new_preds2[cond2] = 0\n",
        "            new_f1b = f1_score(y_hold, new_preds2)\n",
        "            if new_f1b > best_rule[\"new_f1\"]:\n",
        "                best_rule = {\"type\":\"numeric\", \"feature\":col, \"op\":\"<=\", \"threshold\":thresh, \"improvement_f1\": new_f1b - base_f1, \"new_f1\": new_f1b}\n",
        "\n",
        "    # categorical rules: look for categories overrepresented in FN or FP\n",
        "    if categorical_cols:\n",
        "        for col in categorical_cols:\n",
        "            # skip if too many categories\n",
        "            vc = X_hold[col].value_counts(normalize=True)\n",
        "            common_cats = vc[vc.index.notnull()].head(10).index.tolist()\n",
        "            for cat in common_cats:\n",
        "                # rule: when category==cat and pred==0 -> flip to 1\n",
        "                new_preds = base_pred.copy()\n",
        "                cond = (X_hold[col].values == cat) & (base_pred == 0)\n",
        "                if cond.sum() == 0:\n",
        "                    continue\n",
        "                new_preds[cond] = 1\n",
        "                new_f1 = f1_score(y_hold, new_preds)\n",
        "                if new_f1 > best_rule[\"new_f1\"]:\n",
        "                    best_rule = {\"type\":\"categorical\", \"feature\":col, \"value\":cat, \"op\":\"==\", \"improvement_f1\": new_f1 - base_f1, \"new_f1\": new_f1}\n",
        "\n",
        "    # Keep the best rule only if it improves F1\n",
        "    selected_rule = None\n",
        "    if best_rule[\"new_f1\"] > base_f1 + 1e-6:\n",
        "        selected_rule = best_rule\n",
        "\n",
        "    # 8) Probability calibration (on training set) - test if it improves ROC AUC on holdout\n",
        "    calibrated = None\n",
        "    try:\n",
        "        calibrator = CalibratedClassifierCV(base_model, cv=5, method='sigmoid')\n",
        "        calibrator.fit(X_train_proc, y_train)\n",
        "        cal_proba_hold = calibrator.predict_proba(X_hold_proc)[:, 1]\n",
        "        cal_roc = roc_auc_score(y_hold, cal_proba_hold)\n",
        "        raw_roc = roc_auc_score(y_hold, base_proba)\n",
        "        if cal_roc >= raw_roc:\n",
        "            calibrated = calibrator\n",
        "            used_proba_hold = cal_proba_hold\n",
        "            used_proba_test = calibrator.predict_proba(X_test_proc)[:, 1]\n",
        "            calib_used = True\n",
        "        else:\n",
        "            used_proba_hold = base_proba\n",
        "            used_proba_test = None  # compute later from base_model\n",
        "            calib_used = False\n",
        "    except Exception:\n",
        "        used_proba_hold = base_proba\n",
        "        used_proba_test = None\n",
        "        calib_used = False\n",
        "\n",
        "    # 9) Threshold tuning (maximize F1 on holdout) using used_proba_hold (calibrated if used)\n",
        "    probs_for_thresh = used_proba_hold\n",
        "    best_t = 0.5\n",
        "    best_f1_thresh = base_f1\n",
        "    for t in np.linspace(0.1, 0.9, 81):\n",
        "        preds_t = (probs_for_thresh >= t).astype(int)\n",
        "        # if rule selected, apply rule to these preds for evaluation\n",
        "        if selected_rule is not None:\n",
        "            preds_t = apply_rule_to_preds(preds_t, X_hold, selected_rule)\n",
        "        cur_f1 = f1_score(y_hold, preds_t)\n",
        "        if cur_f1 > best_f1_thresh:\n",
        "            best_f1_thresh = cur_f1\n",
        "            best_t = float(t)\n",
        "\n",
        "    # 10) Final model choice: choose between rf/mlp/calibrated using holdout ROC/F1\n",
        "    # compute chosen models' proba on holdout\n",
        "    rf_hold_proba = rf.predict_proba(X_hold_proc)[:, 1]\n",
        "    mlp_hold_proba = mlp.predict_proba(X_hold_proc)[:, 1]\n",
        "    rf_f1 = f1_score(y_hold, (rf_hold_proba >= 0.5).astype(int))\n",
        "    mlp_f1 = f1_score(y_hold, (mlp_hold_proba >= 0.5).astype(int))\n",
        "    # prefer calibrated base if available\n",
        "    candidates = []\n",
        "    candidates.append((\"rf\", rf, rf_hold_proba, rf_f1))\n",
        "    candidates.append((\"mlp\", mlp, mlp_hold_proba, mlp_f1))\n",
        "    if calibrated is not None:\n",
        "        cal_hold_proba = calibrator.predict_proba(X_hold_proc)[:, 1]\n",
        "        cal_f1 = f1_score(y_hold, (cal_hold_proba >= 0.5).astype(int))\n",
        "        candidates.append((\"calibrated_base\", calibrator, cal_hold_proba, cal_f1))\n",
        "    # pick candidate with highest F1 on holdout (after applying selected_rule + threshold tuning)\n",
        "    best_candidate = None\n",
        "    best_candidate_score = -1\n",
        "    for name, model_obj, proba, f1raw in candidates:\n",
        "        # use best_t\n",
        "        preds_candidate = (proba >= best_t).astype(int)\n",
        "        if selected_rule is not None:\n",
        "            preds_candidate = apply_rule_to_preds(preds_candidate, X_hold, selected_rule)\n",
        "        cur_f1 = f1_score(y_hold, preds_candidate)\n",
        "        if cur_f1 > best_candidate_score:\n",
        "            best_candidate_score = cur_f1\n",
        "            best_candidate = {\"name\": name, \"model\": model_obj, \"proba_hold\": proba, \"f1\": cur_f1}\n",
        "\n",
        "    # 11) Final predictions on test\n",
        "    if best_candidate[\"name\"] == \"rf\":\n",
        "        test_proba = rf.predict_proba(X_test_proc)[:, 1]\n",
        "    elif best_candidate[\"name\"] == \"mlp\":\n",
        "        test_proba = mlp.predict_proba(X_test_proc)[:, 1]\n",
        "    else:\n",
        "        test_proba = calibrator.predict_proba(X_test_proc)[:, 1]\n",
        "\n",
        "    test_preds = (test_proba >= best_t).astype(int)\n",
        "    if selected_rule is not None:\n",
        "        test_preds = apply_rule_to_preds(test_preds, X_test, selected_rule)\n",
        "\n",
        "    # 12) Save submission, models, report\n",
        "    submission = pd.DataFrame({ id_col if id_col else \"Id\": test_ids, TARGET: test_preds })\n",
        "    submission.to_csv(OUTPUT_SUB, index=False)\n",
        "\n",
        "    saved_obj = {\n",
        "        \"preprocessor\": preprocessor,\n",
        "        \"rf\": rf,\n",
        "        \"mlp\": mlp,\n",
        "        \"calibrator\": calibrator if 'calibrator' in locals() else None,\n",
        "        \"chosen_model\": best_candidate[\"name\"],\n",
        "        \"selected_rule\": selected_rule,\n",
        "        \"threshold\": best_t\n",
        "    }\n",
        "    joblib.dump(saved_obj, OUTPUT_MODELS)\n",
        "\n",
        "    report = {\n",
        "        \"rf_stats\": rf_stats,\n",
        "        \"mlp_stats\": mlp_stats,\n",
        "        \"chosen_explainability\": \"shap\" if len(shap_top_features)>0 else \"permutation\",\n",
        "        \"shap_top\": shap_top_features[:10] if shap_top_features else [],\n",
        "        \"perm_top\": perm_top_features[:10] if perm_top_features else [],\n",
        "        \"fn_fp_counts\": {\"TP\": int(len(tp)), \"FN\": int(len(fn)), \"TN\": int(len(tn)), \"FP\": int(len(fp))},\n",
        "        \"top_fn_numeric\": fn_diffs_sorted[:10],\n",
        "        \"top_fp_numeric\": fp_diffs_sorted[:10],\n",
        "        \"selected_rule\": selected_rule,\n",
        "        \"best_threshold\": best_t,\n",
        "        \"best_holdout_f1_after_rule\": float(best_candidate[\"f1\"]),\n",
        "        \"notes\": \"If SHAP not available, permutation importance used. Keep an eye on selected_rule generalization to test set.\"\n",
        "    }\n",
        "    with open(OUTPUT_REPORT, \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(\"Done. Outputs:\")\n",
        "    print(\"-\", OUTPUT_SUB)\n",
        "    print(\"-\", OUTPUT_MODELS)\n",
        "    print(\"-\", OUTPUT_REPORT)\n",
        "\n",
        "\n",
        "# Helper function defined after main to keep code flow simple\n",
        "def apply_rule_to_preds(preds, X_df, rule):\n",
        "    \"\"\"Apply single selected_rule to numpy preds array and DataFrame X_df; return modified preds (numpy).\"\"\"\n",
        "    preds = preds.copy()\n",
        "    if rule is None:\n",
        "        return preds\n",
        "    if rule[\"type\"] == \"numeric\":\n",
        "        feat = rule[\"feature\"]\n",
        "        thr = float(rule[\"threshold\"])\n",
        "        if rule[\"op\"] == \">=\":\n",
        "            cond = (X_df[feat].values >= thr) & (preds == 0)\n",
        "            preds[cond] = 1\n",
        "        elif rule[\"op\"] == \"<=\":\n",
        "            cond = (X_df[feat].values <= thr) & (preds == 1)\n",
        "            preds[cond] = 0\n",
        "    elif rule[\"type\"] == \"categorical\":\n",
        "        feat = rule[\"feature\"]\n",
        "        val = rule[\"value\"]\n",
        "        if rule[\"op\"] == \"==\":\n",
        "            # flip 0->1 where category==val\n",
        "            cond = (X_df[feat].values == val) & (preds == 0)\n",
        "            preds[cond] = 1\n",
        "    return preds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}